{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Deep Learning Model ‚Äî CNN + BiLSTM + Multi-Head Attention\n",
    "- Architecture: Conv1D ‚Üí Conv1D ‚Üí BiLSTM ‚Üí Attention ‚Üí Linear(256‚Üí168)\n",
    "- Huber Loss + AdamW + ReduceLROnPlateau + Early Stopping\n",
    "- Gradient clipping, dropout, Gaussian noise\n",
    "- Evaluate on test, horizon-wise error, rolling origin backtesting\n",
    "- Full model comparison with baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, json, time, numpy as np, joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_DIR  = '/content/drive/MyDrive/Electricity_Load_Forecast'\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "config         = joblib.load(os.path.join(MODEL_DIR, 'config.pkl'))\n",
    "target_scaler  = joblib.load(os.path.join(MODEL_DIR, 'target_scaler.pkl'))\n",
    "\n",
    "X_train_w = np.load(os.path.join(MODEL_DIR, 'X_train_w.npy'))\n",
    "y_train_w = np.load(os.path.join(MODEL_DIR, 'y_train_w.npy'))\n",
    "X_val_w   = np.load(os.path.join(MODEL_DIR, 'X_val_w.npy'))\n",
    "y_val_w   = np.load(os.path.join(MODEL_DIR, 'y_val_w.npy'))\n",
    "X_test_w  = np.load(os.path.join(MODEL_DIR, 'X_test_w.npy'))\n",
    "y_test_w  = np.load(os.path.join(MODEL_DIR, 'y_test_w.npy'))\n",
    "\n",
    "N_FEATURES = config['N_FEATURES']\n",
    "INPUT_LEN  = config['INPUT_LEN']\n",
    "OUTPUT_LEN = config['OUTPUT_LEN']\n",
    "print(f\"Train: {X_train_w.shape}, Val: {X_val_w.shape}, Test: {X_test_w.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "HP = {\n",
    "    'batch_size':    64,\n",
    "    'lr':            0.001,\n",
    "    'weight_decay':  1e-4,\n",
    "    'max_epochs':    100,\n",
    "    'patience':      15,       # early stopping\n",
    "    'lr_patience':   5,        # ReduceLROnPlateau\n",
    "    'lr_factor':     0.5,\n",
    "    'grad_clip':     1.0,\n",
    "    'dropout':       0.3,\n",
    "    'noise_std':     0.01,\n",
    "    'conv_filters':  64,\n",
    "    'lstm_hidden':   128,\n",
    "    'n_heads':       4,\n",
    "    'huber_delta':   1.0,\n",
    "}\n",
    "print(\"Hyperparameters:\", json.dumps(HP, indent=2))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def make_loader(X, y, batch_size, shuffle=True):\n",
    "    ds = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=True)\n",
    "\n",
    "train_loader = make_loader(X_train_w, y_train_w, HP['batch_size'], shuffle=True)\n",
    "val_loader   = make_loader(X_val_w,   y_val_w,   HP['batch_size'], shuffle=False)\n",
    "test_loader  = make_loader(X_test_w,  y_test_w,  HP['batch_size'], shuffle=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#  MODEL ARCHITECTURE\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "class LoadForecastModel(nn.Module):\n",
    "    def __init__(self, n_features, seq_len, pred_len, conv_filters=64,\n",
    "                 lstm_hidden=128, n_heads=4, dropout=0.3, noise_std=0.01):\n",
    "        super().__init__()\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Conv1D block 1: kernel=3\n",
    "        self.conv1 = nn.Conv1d(n_features, conv_filters, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm1d(conv_filters)\n",
    "\n",
    "        # Conv1D block 2: kernel=5\n",
    "        self.conv2 = nn.Conv1d(conv_filters, conv_filters, kernel_size=5, padding=2)\n",
    "        self.bn2   = nn.BatchNorm1d(conv_filters)\n",
    "\n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(conv_filters, lstm_hidden, batch_first=True,\n",
    "                              bidirectional=True, num_layers=1)\n",
    "        lstm_out = lstm_hidden * 2  # 256\n",
    "\n",
    "        # Multi-Head Self-Attention\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=lstm_out, num_heads=n_heads,\n",
    "                                               batch_first=True, dropout=dropout)\n",
    "\n",
    "        # LayerNorm + Dropout\n",
    "        self.layer_norm = nn.LayerNorm(lstm_out)\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "\n",
    "        # Output: pool ‚Üí Linear(256 ‚Üí 168)\n",
    "        self.fc = nn.Linear(lstm_out, pred_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, seq_len, n_features)\n",
    "\n",
    "        # Gaussian Noise (training only)\n",
    "        if self.training and self.noise_std > 0:\n",
    "            x = x + torch.randn_like(x) * self.noise_std\n",
    "\n",
    "        # Conv1D expects (B, C, L)\n",
    "        x = x.permute(0, 2, 1)                    # (B, n_features, seq_len)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))        # (B, 64, seq_len)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))        # (B, 64, seq_len)\n",
    "        x = x.permute(0, 2, 1)                    # (B, seq_len, 64)\n",
    "\n",
    "        # BiLSTM\n",
    "        x, _ = self.bilstm(x)                     # (B, seq_len, 256)\n",
    "\n",
    "        # Multi-Head Self-Attention + residual\n",
    "        attn_out, _ = self.attention(x, x, x)      # (B, seq_len, 256)\n",
    "        x = self.layer_norm(x + attn_out)           # residual + norm\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = x.mean(dim=1)                          # (B, 256)\n",
    "\n",
    "        # Direct multi-step forecast\n",
    "        x = self.fc(x)                             # (B, 168)\n",
    "        return x\n",
    "\n",
    "model = LoadForecastModel(\n",
    "    n_features=N_FEATURES, seq_len=INPUT_LEN, pred_len=OUTPUT_LEN,\n",
    "    conv_filters=HP['conv_filters'], lstm_hidden=HP['lstm_hidden'],\n",
    "    n_heads=HP['n_heads'], dropout=HP['dropout'], noise_std=HP['noise_std']\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#  TRAINING LOOP\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "criterion = nn.HuberLoss(delta=HP['huber_delta'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=HP['lr'], weight_decay=HP['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=HP['lr_patience'], factor=HP['lr_factor'], verbose=True\n",
    ")\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = os.path.join(MODEL_DIR, 'best_dl_model.pt')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training for up to {HP['max_epochs']} epochs (early stop patience={HP['patience']})\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for epoch in range(HP['max_epochs']):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ‚Äî Train ‚Äî\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for X_b, y_b in train_loader:\n",
    "        X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_b)\n",
    "        loss = criterion(pred, y_b)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=HP['grad_clip'])\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    train_loss = np.mean(train_losses)\n",
    "\n",
    "    # ‚Äî Validate ‚Äî\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in val_loader:\n",
    "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "            pred = model(X_b)\n",
    "            loss = criterion(pred, y_b)\n",
    "            val_losses.append(loss.item())\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    # ‚Äî Scheduler ‚Äî\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['lr'].append(current_lr)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"Epoch {epoch+1:3d}/{HP['max_epochs']} | \"\n",
    "          f\"Train: {train_loss:.6f} | Val: {val_loss:.6f} | \"\n",
    "          f\"LR: {current_lr:.2e} | {elapsed:.1f}s\")\n",
    "\n",
    "    # ‚Äî Early Stopping ‚Äî\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= HP['patience']:\n",
    "            print(f\"\\n‚õî Early stopping at epoch {epoch+1} (best val loss: {best_val_loss:.6f})\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print(f\"‚úÖ Best model loaded (val loss: {best_val_loss:.6f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.plot(history['train_loss'], label='Train Loss')\n",
    "ax1.plot(history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Huber Loss'); ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history['lr'], color='green')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Learning Rate'); ax2.set_title('Learning Rate Schedule')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'dl_training_history.png'), dpi=150)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#  TEST EVALUATION\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "def predict_all(model, loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_b, _ in loader:\n",
    "            preds.append(model(X_b.to(device)).cpu().numpy())\n",
    "    return np.concatenate(preds, axis=0)\n",
    "\n",
    "def inverse_scale(y):\n",
    "    return target_scaler.inverse_transform(y.reshape(-1, 1)).reshape(y.shape)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    yt, yp = inverse_scale(y_true), inverse_scale(y_pred)\n",
    "    mae  = mean_absolute_error(yt.flatten(), yp.flatten())\n",
    "    rmse = np.sqrt(mean_squared_error(yt.flatten(), yp.flatten()))\n",
    "    mask = yt.flatten() != 0\n",
    "    mape = np.mean(np.abs((yt.flatten()[mask] - yp.flatten()[mask]) / yt.flatten()[mask])) * 100\n",
    "    peak_errors = [np.abs(yt[i, np.argmax(yt[i])] - yp[i, np.argmax(yt[i])]) for i in range(len(yt))]\n",
    "    peak_mae = np.mean(peak_errors)\n",
    "    return {'MAE': round(mae, 2), 'RMSE': round(rmse, 2),\n",
    "            'MAPE': round(mape, 2), 'Peak_MAE': round(peak_mae, 2)}\n",
    "\n",
    "y_pred_test = predict_all(model, test_loader)\n",
    "y_pred_val  = predict_all(model, val_loader)\n",
    "\n",
    "dl_val  = compute_metrics(y_val_w,  y_pred_val)\n",
    "dl_test = compute_metrics(y_test_w, y_pred_test)\n",
    "\n",
    "print(f\"\\n{'='*40}\\nDL Model ‚Äî Validation\\n{'='*40}\")\n",
    "for k, v in dl_val.items():  print(f\"  {k:10s}: {v}\")\n",
    "print(f\"\\n{'='*40}\\nDL Model ‚Äî Test\\n{'='*40}\")\n",
    "for k, v in dl_test.items(): print(f\"  {k:10s}: {v}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "horizons = [1, 24, 72, 168]\n",
    "yt_inv = inverse_scale(y_test_w)\n",
    "yp_inv = inverse_scale(y_pred_test)\n",
    "\n",
    "horizon_metrics = {}\n",
    "for h in horizons:\n",
    "    idx = h - 1  # 0-indexed\n",
    "    mae_h  = mean_absolute_error(yt_inv[:, idx], yp_inv[:, idx])\n",
    "    rmse_h = np.sqrt(mean_squared_error(yt_inv[:, idx], yp_inv[:, idx]))\n",
    "    mask = yt_inv[:, idx] != 0\n",
    "    mape_h = np.mean(np.abs((yt_inv[:, idx][mask] - yp_inv[:, idx][mask]) / yt_inv[:, idx][mask])) * 100\n",
    "    horizon_metrics[h] = {'MAE': round(mae_h, 2), 'RMSE': round(rmse_h, 2), 'MAPE': round(mape_h, 2)}\n",
    "\n",
    "print(f\"\\n{'='*50}\\nHorizon-Wise Error (Test Set)\\n{'='*50}\")\n",
    "print(f\"{'Hour':>6} {'MAE':>10} {'RMSE':>10} {'MAPE%':>10}\")\n",
    "for h, m in horizon_metrics.items():\n",
    "    print(f\"{h:>6} {m['MAE']:>10} {m['RMSE']:>10} {m['MAPE']:>10}\")\n",
    "\n",
    "# Horizon error curve (all hours)\n",
    "mae_per_hour = [mean_absolute_error(yt_inv[:, h], yp_inv[:, h]) for h in range(OUTPUT_LEN)]\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, OUTPUT_LEN+1), mae_per_hour, 'b-', lw=1.5)\n",
    "for h in horizons:\n",
    "    plt.axvline(h, color='red', ls='--', alpha=0.5)\n",
    "    plt.annotate(f'h={h}', (h, mae_per_hour[h-1]), fontsize=9, color='red')\n",
    "plt.xlabel('Forecast Horizon (hours)'); plt.ylabel('MAE (MW)')\n",
    "plt.title('Horizon-Wise MAE ‚Äî DL Model (Test Set)'); plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'horizon_wise_error.png'), dpi=150)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n_origins = 10\n",
    "step = max(1, len(X_test_w) // n_origins)\n",
    "origin_indices = list(range(0, len(X_test_w), step))[:n_origins]\n",
    "\n",
    "rolling_metrics = []\n",
    "for idx in origin_indices:\n",
    "    X_single = torch.tensor(X_test_w[idx:idx+1]).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_single).cpu().numpy()\n",
    "    true = y_test_w[idx:idx+1]\n",
    "    m = compute_metrics(true, pred)\n",
    "    m['origin_idx'] = idx\n",
    "    rolling_metrics.append(m)\n",
    "\n",
    "print(f\"\\n{'='*60}\\nRolling Origin Evaluation ({n_origins} origins)\\n{'='*60}\")\n",
    "print(f\"{'Origin':>8} {'MAE':>8} {'RMSE':>8} {'MAPE%':>8} {'PeakMAE':>10}\")\n",
    "for rm in rolling_metrics:\n",
    "    print(f\"{rm['origin_idx']:>8} {rm['MAE']:>8} {rm['RMSE']:>8} {rm['MAPE']:>8} {rm['Peak_MAE']:>10}\")\n",
    "\n",
    "avg_rolling = {k: round(np.mean([m[k] for m in rolling_metrics]), 2)\n",
    "               for k in ['MAE','RMSE','MAPE','Peak_MAE']}\n",
    "print(f\"{'AVG':>8} {avg_rolling['MAE']:>8} {avg_rolling['RMSE']:>8} \"\n",
    "      f\"{avg_rolling['MAPE']:>8} {avg_rolling['Peak_MAE']:>10}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#  FULL MODEL COMPARISON\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Load baseline metrics\n",
    "with open(os.path.join(MODEL_DIR, 'baseline_metrics.json'), 'r') as f:\n",
    "    baseline_metrics = json.load(f)\n",
    "\n",
    "all_metrics = {\n",
    "    'Persistence': baseline_metrics['Persistence'],\n",
    "    'XGBoost':     baseline_metrics['XGBoost'],\n",
    "    'CNN-BiLSTM-Attn': {'val': dl_val, 'test': dl_test}\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"   FULL MODEL COMPARISON\")\n",
    "print(f\"{'='*70}\")\n",
    "for split_name in ['val', 'test']:\n",
    "    print(f\"\\n--- {split_name.upper()} SET ---\")\n",
    "    print(f\"{'Model':<20} {'MAE':>8} {'RMSE':>8} {'MAPE%':>8} {'PeakMAE':>10}\")\n",
    "    print(\"-\"*60)\n",
    "    for model_name, m in all_metrics.items():\n",
    "        t = m[split_name]\n",
    "        print(f\"{model_name:<20} {t['MAE']:>8} {t['RMSE']:>8} {t['MAPE']:>8} {t['Peak_MAE']:>10}\")\n",
    "\n",
    "# Comparison bar chart\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "metric_names = ['MAE', 'RMSE', 'MAPE', 'Peak_MAE']\n",
    "model_names = list(all_metrics.keys())\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "\n",
    "for ax, metric in zip(axes, metric_names):\n",
    "    vals = [all_metrics[m]['test'][metric] for m in model_names]\n",
    "    bars = ax.bar(model_names, vals, color=colors, edgecolor='white', linewidth=1.5)\n",
    "    ax.set_title(metric, fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{v}', ha='center', fontsize=10)\n",
    "    ax.grid(True, alpha=0.2, axis='y')\n",
    "\n",
    "plt.suptitle('Model Comparison ‚Äî Test Set', fontsize=15, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'model_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DL metrics\n",
    "dl_metrics_full = {\n",
    "    'val': dl_val, 'test': dl_test,\n",
    "    'horizon_wise': {str(k): v for k, v in horizon_metrics.items()},\n",
    "    'rolling_origin': {'per_origin': rolling_metrics, 'average': avg_rolling},\n",
    "    'hyperparameters': HP,\n",
    "    'total_params': total_params,\n",
    "    'best_val_loss': best_val_loss\n",
    "}\n",
    "with open(os.path.join(MODEL_DIR, 'dl_metrics.json'), 'w') as f:\n",
    "    json.dump(dl_metrics_full, f, indent=2)\n",
    "\n",
    "# Training history\n",
    "import pandas as pd\n",
    "pd.DataFrame(history).to_csv(os.path.join(MODEL_DIR, 'dl_training_history.csv'), index=False)\n",
    "\n",
    "# Full comparison\n",
    "with open(os.path.join(MODEL_DIR, 'all_model_comparison.json'), 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ All saved to models/:\")\n",
    "print(\"  ‚Ä¢ best_dl_model.pt\")\n",
    "print(\"  ‚Ä¢ dl_metrics.json\")\n",
    "print(\"  ‚Ä¢ dl_training_history.csv\")\n",
    "print(\"  ‚Ä¢ dl_training_history.png\")\n",
    "print(\"  ‚Ä¢ horizon_wise_error.png\")\n",
    "print(\"  ‚Ä¢ model_comparison.png\")\n",
    "print(\"  ‚Ä¢ all_model_comparison.json\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    si = idx * (len(y_test_w) // 4)\n",
    "    ax.plot(inverse_scale(y_test_w[si]), 'k-', lw=2, label='Actual')\n",
    "    ax.plot(inverse_scale(y_pred_test[si]), 'g--', lw=1.5, label='DL Prediction')\n",
    "    ax.set_title(f'Test Window #{si}'); ax.set_xlabel('Hour'); ax.set_ylabel('Load (MW)')\n",
    "    ax.legend(); ax.grid(True, alpha=0.3)\n",
    "plt.suptitle('CNN-BiLSTM-Attention Predictions vs Actual', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'dl_sample_predictions.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"üéâ All done!\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
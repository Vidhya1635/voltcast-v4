{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ§  Deep Learning Model V2 â€” Improved CNN + BiLSTM + Attention\n",
                "### Key improvements over V1:\n",
                "1. **Multi-scale CNN** â€” parallel Conv1d with kernels 3, 7, 13 to capture hourly + daily + weekly patterns\n",
                "2. **Deeper BiLSTM** â€” 2-layer stacked BiLSTM with residual connections\n",
                "3. **Temporal Decoder** â€” learned upsampling from compressed representation instead of global avg pool â†’ single linear\n",
                "4. **Peak-Aware Loss** â€” Huber + weighted penalty on top-K load hours\n",
                "5. **Stronger Regularization** â€” spatial dropout, weight decay, mixup augmentation\n",
                "6. **Cosine Annealing with Warmup** â€” better LR schedule than ReduceLROnPlateau\n",
                "7. **Gradient accumulation** â€” effective batch size 256 for stable training\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import os, json, time, math, numpy as np, joblib\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "import warnings; warnings.filterwarnings('ignore')\n",
                "\n",
                "# Custom JSON encoder for numpy types\n",
                "class NumpyEncoder(json.JSONEncoder):\n",
                "    def default(self, obj):\n",
                "        if isinstance(obj, (np.integer,)):\n",
                "            return int(obj)\n",
                "        if isinstance(obj, (np.floating,)):\n",
                "            return float(obj)\n",
                "        if isinstance(obj, np.ndarray):\n",
                "            return obj.tolist()\n",
                "        return super().default(obj)\n",
                "\n",
                "BASE_DIR  = '/content/drive/MyDrive/Electricity_Load_Forecast'\n",
                "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
                "V2_DIR    = os.path.join(MODEL_DIR, 'v2')\n",
                "os.makedirs(V2_DIR, exist_ok=True)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "config         = joblib.load(os.path.join(MODEL_DIR, 'config.pkl'))\n",
                "target_scaler  = joblib.load(os.path.join(MODEL_DIR, 'target_scaler.pkl'))\n",
                "\n",
                "X_train_w = np.load(os.path.join(MODEL_DIR, 'X_train_w.npy'))\n",
                "y_train_w = np.load(os.path.join(MODEL_DIR, 'y_train_w.npy'))\n",
                "X_val_w   = np.load(os.path.join(MODEL_DIR, 'X_val_w.npy'))\n",
                "y_val_w   = np.load(os.path.join(MODEL_DIR, 'y_val_w.npy'))\n",
                "X_test_w  = np.load(os.path.join(MODEL_DIR, 'X_test_w.npy'))\n",
                "y_test_w  = np.load(os.path.join(MODEL_DIR, 'y_test_w.npy'))\n",
                "\n",
                "N_FEATURES = config['N_FEATURES']\n",
                "INPUT_LEN  = config['INPUT_LEN']\n",
                "OUTPUT_LEN = config['OUTPUT_LEN']\n",
                "print(f\"Features: {N_FEATURES}, Input: {INPUT_LEN}h, Output: {OUTPUT_LEN}h\")\n",
                "print(f\"Train: {X_train_w.shape}, Val: {X_val_w.shape}, Test: {X_test_w.shape}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "HP = {\n",
                "    'batch_size':      64,\n",
                "    'accum_steps':     4,\n",
                "    'lr':              3e-4,\n",
                "    'weight_decay':    1e-3,\n",
                "    'max_epochs':      80,\n",
                "    'patience':        20,\n",
                "    'warmup_epochs':   5,\n",
                "    'grad_clip':       0.5,\n",
                "    'dropout':         0.2,\n",
                "    'spatial_dropout':  0.1,\n",
                "    'noise_std':       0.02,\n",
                "    'mixup_alpha':     0.2,\n",
                "    'conv_filters':    64,\n",
                "    'lstm_hidden':     192,\n",
                "    'lstm_layers':     2,\n",
                "    'n_heads':         8,\n",
                "    'decoder_hidden':  256,\n",
                "    'huber_delta':     1.0,\n",
                "    'peak_weight':     2.0,\n",
                "    'peak_fraction':   0.2,\n",
                "}\n",
                "print(\"Hyperparameters:\", json.dumps(HP, indent=2))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def make_loader(X, y, batch_size, shuffle=True):\n",
                "    ds = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
                "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle,\n",
                "                      num_workers=2, pin_memory=True, drop_last=shuffle)\n",
                "\n",
                "train_loader = make_loader(X_train_w, y_train_w, HP['batch_size'], shuffle=True)\n",
                "val_loader   = make_loader(X_val_w,   y_val_w,   HP['batch_size'], shuffle=False)\n",
                "test_loader  = make_loader(X_test_w,  y_test_w,  HP['batch_size'], shuffle=False)\n",
                "print(f\"Train batches: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  IMPROVED MODEL ARCHITECTURE V2\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "class MultiScaleCNN(nn.Module):\n",
                "    \"\"\"Parallel Conv1d with different kernel sizes to capture multi-scale patterns.\"\"\"\n",
                "    def __init__(self, in_channels, out_channels, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.conv3  = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
                "        self.conv7  = nn.Conv1d(in_channels, out_channels, kernel_size=7, padding=3)\n",
                "        self.conv13 = nn.Conv1d(in_channels, out_channels, kernel_size=13, padding=6)\n",
                "        self.bn     = nn.BatchNorm1d(out_channels * 3)\n",
                "        self.proj   = nn.Conv1d(out_channels * 3, out_channels, kernel_size=1)\n",
                "        self.bn2    = nn.BatchNorm1d(out_channels)\n",
                "        self.drop   = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        c3  = F.gelu(self.conv3(x))\n",
                "        c7  = F.gelu(self.conv7(x))\n",
                "        c13 = F.gelu(self.conv13(x))\n",
                "        out = torch.cat([c3, c7, c13], dim=1)\n",
                "        out = F.gelu(self.bn(out))\n",
                "        out = self.drop(F.gelu(self.bn2(self.proj(out))))\n",
                "        return out\n",
                "\n",
                "\n",
                "class SpatialDropout1d(nn.Module):\n",
                "    def __init__(self, p=0.1):\n",
                "        super().__init__()\n",
                "        self.p = p\n",
                "\n",
                "    def forward(self, x):\n",
                "        if not self.training or self.p == 0:\n",
                "            return x\n",
                "        mask = torch.bernoulli(torch.ones(x.shape[0], 1, x.shape[2], device=x.device) * (1 - self.p))\n",
                "        return x * mask / (1 - self.p)\n",
                "\n",
                "\n",
                "class TemporalDecoder(nn.Module):\n",
                "    \"\"\"Learned temporal queries + cross-attention decoder.\"\"\"\n",
                "    def __init__(self, d_model, pred_len, n_heads=8, dropout=0.2):\n",
                "        super().__init__()\n",
                "        self.pred_len = pred_len\n",
                "        self.query_embed = nn.Parameter(torch.randn(1, pred_len, d_model) * 0.02)\n",
                "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
                "        self.norm1 = nn.LayerNorm(d_model)\n",
                "        self.ffn = nn.Sequential(\n",
                "            nn.Linear(d_model, d_model * 2), nn.GELU(), nn.Dropout(dropout),\n",
                "            nn.Linear(d_model * 2, d_model), nn.Dropout(dropout)\n",
                "        )\n",
                "        self.norm2 = nn.LayerNorm(d_model)\n",
                "        self.out_proj = nn.Linear(d_model, 1)\n",
                "\n",
                "    def forward(self, encoder_out):\n",
                "        B = encoder_out.shape[0]\n",
                "        queries = self.query_embed.expand(B, -1, -1)\n",
                "        attn_out, _ = self.cross_attn(queries, encoder_out, encoder_out)\n",
                "        x = self.norm1(queries + attn_out)\n",
                "        x = self.norm2(x + self.ffn(x))\n",
                "        x = self.out_proj(x).squeeze(-1)\n",
                "        return x\n",
                "\n",
                "\n",
                "class LoadForecastV2(nn.Module):\n",
                "    def __init__(self, n_features, seq_len, pred_len,\n",
                "                 conv_filters=64, lstm_hidden=192, lstm_layers=2,\n",
                "                 n_heads=8, decoder_hidden=256,\n",
                "                 dropout=0.2, spatial_drop=0.1, noise_std=0.02):\n",
                "        super().__init__()\n",
                "        self.noise_std = noise_std\n",
                "        self.input_proj = nn.Linear(n_features, conv_filters)\n",
                "        self.spatial_drop = SpatialDropout1d(spatial_drop)\n",
                "        self.ms_cnn = MultiScaleCNN(conv_filters, conv_filters, dropout=dropout)\n",
                "        self.conv_block2 = nn.Sequential(\n",
                "            nn.Conv1d(conv_filters, conv_filters, kernel_size=5, padding=2),\n",
                "            nn.BatchNorm1d(conv_filters), nn.GELU(), nn.Dropout(dropout)\n",
                "        )\n",
                "        d_model = lstm_hidden * 2\n",
                "        self.bilstm1 = nn.LSTM(conv_filters, lstm_hidden, batch_first=True, bidirectional=True)\n",
                "        self.bilstm2 = nn.LSTM(d_model, lstm_hidden, batch_first=True, bidirectional=True)\n",
                "        self.lstm_norm1 = nn.LayerNorm(d_model)\n",
                "        self.lstm_norm2 = nn.LayerNorm(d_model)\n",
                "        self.lstm_drop  = nn.Dropout(dropout)\n",
                "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
                "        self.attn_norm = nn.LayerNorm(d_model)\n",
                "        self.attn_ffn  = nn.Sequential(\n",
                "            nn.Linear(d_model, d_model * 2), nn.GELU(),\n",
                "            nn.Dropout(dropout), nn.Linear(d_model * 2, d_model)\n",
                "        )\n",
                "        self.attn_ffn_norm = nn.LayerNorm(d_model)\n",
                "        self.decoder = TemporalDecoder(d_model, pred_len, n_heads, dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        if self.training and self.noise_std > 0:\n",
                "            x = x + torch.randn_like(x) * self.noise_std\n",
                "        x = self.input_proj(x)\n",
                "        x = self.spatial_drop(x)\n",
                "        x = x.permute(0, 2, 1)\n",
                "        x = self.ms_cnn(x)\n",
                "        x = x + self.conv_block2(x)\n",
                "        x = x.permute(0, 2, 1)\n",
                "        h1, _ = self.bilstm1(x)\n",
                "        h1 = self.lstm_norm1(h1)\n",
                "        h1 = self.lstm_drop(h1)\n",
                "        h2, _ = self.bilstm2(h1)\n",
                "        h2 = self.lstm_norm2(h1 + h2)\n",
                "        attn_out, _ = self.self_attn(h2, h2, h2)\n",
                "        h2 = self.attn_norm(h2 + attn_out)\n",
                "        h2 = self.attn_ffn_norm(h2 + self.attn_ffn(h2))\n",
                "        out = self.decoder(h2)\n",
                "        return out\n",
                "\n",
                "\n",
                "model = LoadForecastV2(\n",
                "    n_features=N_FEATURES, seq_len=INPUT_LEN, pred_len=OUTPUT_LEN,\n",
                "    conv_filters=HP['conv_filters'], lstm_hidden=HP['lstm_hidden'],\n",
                "    lstm_layers=HP['lstm_layers'], n_heads=HP['n_heads'],\n",
                "    decoder_hidden=HP['decoder_hidden'],\n",
                "    dropout=HP['dropout'], spatial_drop=HP['spatial_dropout'],\n",
                "    noise_std=HP['noise_std']\n",
                ").to(device)\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable    = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"Total params: {total_params:,} | Trainable: {trainable:,}\")\n",
                "print(model)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  PEAK-AWARE LOSS + UTILITIES\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class PeakAwareLoss(nn.Module):\n",
                "    def __init__(self, delta=1.0, peak_weight=2.0, peak_fraction=0.2):\n",
                "        super().__init__()\n",
                "        self.huber = nn.HuberLoss(delta=delta, reduction='none')\n",
                "        self.peak_weight = peak_weight\n",
                "        self.peak_fraction = peak_fraction\n",
                "\n",
                "    def forward(self, pred, target):\n",
                "        base_loss = self.huber(pred, target)\n",
                "        k = max(1, int(target.shape[1] * self.peak_fraction))\n",
                "        topk_vals, _ = torch.topk(target, k, dim=1)\n",
                "        threshold = topk_vals[:, -1:].detach()\n",
                "        is_peak = (target >= threshold).float()\n",
                "        weights = 1.0 + (self.peak_weight - 1.0) * is_peak\n",
                "        return (base_loss * weights).mean()\n",
                "\n",
                "\n",
                "def mixup_data(x, y, alpha=0.2):\n",
                "    if alpha <= 0:\n",
                "        return x, y\n",
                "    lam = np.random.beta(alpha, alpha)\n",
                "    lam = max(lam, 1 - lam)\n",
                "    idx = torch.randperm(x.size(0), device=x.device)\n",
                "    return lam * x + (1 - lam) * x[idx], lam * y + (1 - lam) * y[idx]\n",
                "\n",
                "\n",
                "def cosine_warmup_scheduler(optimizer, warmup_epochs, max_epochs, steps_per_epoch):\n",
                "    warmup_steps = warmup_epochs * steps_per_epoch\n",
                "    total_steps  = max_epochs * steps_per_epoch\n",
                "    def lr_lambda(step):\n",
                "        if step < warmup_steps:\n",
                "            return step / max(1, warmup_steps)\n",
                "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
                "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
                "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
                "\n",
                "print(\"âœ… Loss, mixup, and scheduler defined\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  TRAINING LOOP V2\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "criterion = PeakAwareLoss(delta=HP['huber_delta'], peak_weight=HP['peak_weight'],\n",
                "                          peak_fraction=HP['peak_fraction'])\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=HP['lr'], weight_decay=HP['weight_decay'])\n",
                "scheduler = cosine_warmup_scheduler(optimizer, HP['warmup_epochs'], HP['max_epochs'], len(train_loader))\n",
                "\n",
                "history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
                "best_val_loss = float('inf')\n",
                "patience_counter = 0\n",
                "best_model_path = os.path.join(V2_DIR, 'best_dl_model_v2.pt')\n",
                "ACCUM = HP['accum_steps']\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"Training V2 for up to {HP['max_epochs']} epochs (effective BS={HP['batch_size']*ACCUM})\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "for epoch in range(HP['max_epochs']):\n",
                "    t0 = time.time()\n",
                "    model.train()\n",
                "    train_losses = []\n",
                "    optimizer.zero_grad()\n",
                "    for step, (X_b, y_b) in enumerate(train_loader):\n",
                "        X_b, y_b = X_b.to(device), y_b.to(device)\n",
                "        X_b, y_b = mixup_data(X_b, y_b, HP['mixup_alpha'])\n",
                "        pred = model(X_b)\n",
                "        loss = criterion(pred, y_b) / ACCUM\n",
                "        loss.backward()\n",
                "        if (step + 1) % ACCUM == 0 or (step + 1) == len(train_loader):\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), HP['grad_clip'])\n",
                "            optimizer.step()\n",
                "            optimizer.zero_grad()\n",
                "        scheduler.step()\n",
                "        train_losses.append(loss.item() * ACCUM)\n",
                "    train_loss = np.mean(train_losses)\n",
                "\n",
                "    model.eval()\n",
                "    val_losses = []\n",
                "    with torch.no_grad():\n",
                "        for X_b, y_b in val_loader:\n",
                "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
                "            val_losses.append(criterion(model(X_b), y_b).item())\n",
                "    val_loss = np.mean(val_losses)\n",
                "\n",
                "    current_lr = optimizer.param_groups[0]['lr']\n",
                "    history['train_loss'].append(float(train_loss))\n",
                "    history['val_loss'].append(float(val_loss))\n",
                "    history['lr'].append(float(current_lr))\n",
                "\n",
                "    elapsed = time.time() - t0\n",
                "    tag = ''\n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save(model.state_dict(), best_model_path)\n",
                "        tag = ' âœ… BEST'\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "\n",
                "    print(f\"Epoch {epoch+1:3d}/{HP['max_epochs']} | \"\n",
                "          f\"Train: {train_loss:.6f} | Val: {val_loss:.6f} | \"\n",
                "          f\"LR: {current_lr:.2e} | {elapsed:.1f}s{tag}\")\n",
                "\n",
                "    if patience_counter >= HP['patience']:\n",
                "        print(f\"\\nâ›” Early stopping at epoch {epoch+1} (best val: {best_val_loss:.6f})\")\n",
                "        break\n",
                "\n",
                "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
                "print(f\"âœ… Best V2 model loaded (val loss: {best_val_loss:.6f})\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "ax1.plot(history['train_loss'], label='Train Loss', color='#2196F3')\n",
                "ax1.plot(history['val_loss'], label='Val Loss', color='#FF5722')\n",
                "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Peak-Aware Huber Loss')\n",
                "ax1.set_title('Training & Validation Loss (V2)'); ax1.legend(); ax1.grid(True, alpha=0.3)\n",
                "ax2.plot(history['lr'], color='green')\n",
                "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Learning Rate')\n",
                "ax2.set_title('Cosine Warmup Schedule'); ax2.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(V2_DIR, 'dl_training_history_v2.png'), dpi=150)\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  TEST EVALUATION\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "def predict_all(model, loader):\n",
                "    model.eval()\n",
                "    preds = []\n",
                "    with torch.no_grad():\n",
                "        for X_b, _ in loader:\n",
                "            preds.append(model(X_b.to(device)).cpu().numpy())\n",
                "    return np.concatenate(preds, axis=0)\n",
                "\n",
                "def inverse_scale(y):\n",
                "    return target_scaler.inverse_transform(y.reshape(-1, 1)).reshape(y.shape)\n",
                "\n",
                "def compute_metrics(y_true, y_pred):\n",
                "    yt, yp = inverse_scale(y_true), inverse_scale(y_pred)\n",
                "    mae  = float(mean_absolute_error(yt.flatten(), yp.flatten()))\n",
                "    rmse = float(np.sqrt(mean_squared_error(yt.flatten(), yp.flatten())))\n",
                "    mask = yt.flatten() != 0\n",
                "    mape = float(np.mean(np.abs((yt.flatten()[mask] - yp.flatten()[mask]) / yt.flatten()[mask])) * 100)\n",
                "    peak_errors = [float(np.abs(yt[i, np.argmax(yt[i])] - yp[i, np.argmax(yt[i])])) for i in range(len(yt))]\n",
                "    peak_mae = float(np.mean(peak_errors))\n",
                "    return {'MAE': round(mae, 2), 'RMSE': round(rmse, 2),\n",
                "            'MAPE': round(mape, 2), 'Peak_MAE': round(peak_mae, 2)}\n",
                "\n",
                "y_pred_test = predict_all(model, test_loader)\n",
                "y_pred_val  = predict_all(model, val_loader)\n",
                "\n",
                "dl_v2_val  = compute_metrics(y_val_w[:len(y_pred_val)],   y_pred_val)\n",
                "dl_v2_test = compute_metrics(y_test_w[:len(y_pred_test)], y_pred_test)\n",
                "\n",
                "print(f\"\\n{'='*40}\\nDL V2 Model â€” Validation\\n{'='*40}\")\n",
                "for k, v in dl_v2_val.items():  print(f\"  {k:10s}: {v}\")\n",
                "print(f\"\\n{'='*40}\\nDL V2 Model â€” Test\\n{'='*40}\")\n",
                "for k, v in dl_v2_test.items(): print(f\"  {k:10s}: {v}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "horizons = [1, 24, 72, 168]\n",
                "yt_inv = inverse_scale(y_test_w[:len(y_pred_test)])\n",
                "yp_inv = inverse_scale(y_pred_test)\n",
                "\n",
                "horizon_metrics = {}\n",
                "for h in horizons:\n",
                "    idx = h - 1\n",
                "    mae_h  = float(mean_absolute_error(yt_inv[:, idx], yp_inv[:, idx]))\n",
                "    rmse_h = float(np.sqrt(mean_squared_error(yt_inv[:, idx], yp_inv[:, idx])))\n",
                "    mask = yt_inv[:, idx] != 0\n",
                "    mape_h = float(np.mean(np.abs((yt_inv[:, idx][mask] - yp_inv[:, idx][mask]) / yt_inv[:, idx][mask])) * 100)\n",
                "    horizon_metrics[h] = {'MAE': round(mae_h, 2), 'RMSE': round(rmse_h, 2), 'MAPE': round(mape_h, 2)}\n",
                "\n",
                "print(f\"\\n{'='*50}\\nHorizon-Wise Error V2 (Test Set)\\n{'='*50}\")\n",
                "print(f\"{'Hour':>6} {'MAE':>10} {'RMSE':>10} {'MAPE%':>10}\")\n",
                "for h, m in horizon_metrics.items():\n",
                "    print(f\"{h:>6} {m['MAE']:>10} {m['RMSE']:>10} {m['MAPE']:>10}\")\n",
                "\n",
                "mae_per_hour = [float(mean_absolute_error(yt_inv[:, h], yp_inv[:, h])) for h in range(OUTPUT_LEN)]\n",
                "plt.figure(figsize=(12, 5))\n",
                "plt.plot(range(1, OUTPUT_LEN+1), mae_per_hour, 'b-', lw=1.5)\n",
                "for h in horizons:\n",
                "    plt.axvline(h, color='red', ls='--', alpha=0.5)\n",
                "    plt.annotate(f'h={h}', (h, mae_per_hour[h-1]), fontsize=9, color='red')\n",
                "plt.xlabel('Forecast Horizon (hours)'); plt.ylabel('MAE (MW)')\n",
                "plt.title('Horizon-Wise MAE â€” DL V2 (Test Set)'); plt.grid(True, alpha=0.3)\n",
                "plt.savefig(os.path.join(V2_DIR, 'horizon_wise_error_v2.png'), dpi=150)\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "n_origins = 10\n",
                "step = max(1, len(y_pred_test) // n_origins)\n",
                "origin_indices = list(range(0, len(y_pred_test), step))[:n_origins]\n",
                "\n",
                "rolling_metrics = []\n",
                "for idx in origin_indices:\n",
                "    X_single = torch.tensor(X_test_w[idx:idx+1]).to(device)\n",
                "    with torch.no_grad():\n",
                "        pred = model(X_single).cpu().numpy()\n",
                "    m = compute_metrics(y_test_w[idx:idx+1], pred)\n",
                "    m['origin_idx'] = int(idx)\n",
                "    rolling_metrics.append(m)\n",
                "\n",
                "print(f\"\\n{'='*60}\\nRolling Origin Evaluation V2 ({n_origins} origins)\\n{'='*60}\")\n",
                "print(f\"{'Origin':>8} {'MAE':>8} {'RMSE':>8} {'MAPE%':>8} {'PeakMAE':>10}\")\n",
                "for rm in rolling_metrics:\n",
                "    print(f\"{rm['origin_idx']:>8} {rm['MAE']:>8} {rm['RMSE']:>8} {rm['MAPE']:>8} {rm['Peak_MAE']:>10}\")\n",
                "\n",
                "avg_rolling = {k: round(float(np.mean([m[k] for m in rolling_metrics])), 2)\n",
                "               for k in ['MAE','RMSE','MAPE','Peak_MAE']}\n",
                "print(f\"{'AVG':>8} {avg_rolling['MAE']:>8} {avg_rolling['RMSE']:>8} \"\n",
                "      f\"{avg_rolling['MAPE']:>8} {avg_rolling['Peak_MAE']:>10}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  FULL MODEL COMPARISON\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "with open(os.path.join(MODEL_DIR, 'baseline_metrics.json'), 'r') as f:\n",
                "    baseline_metrics = json.load(f)\n",
                "with open(os.path.join(MODEL_DIR, 'dl_metrics.json'), 'r') as f:\n",
                "    v1_dl = json.load(f)\n",
                "\n",
                "all_metrics = {\n",
                "    'Persistence':   baseline_metrics['Persistence'],\n",
                "    'XGBoost':       baseline_metrics['XGBoost'],\n",
                "    'CNN-BiLSTM V1': v1_dl.get('CNN-BiLSTM-Attn', {'val': v1_dl.get('val',{}), 'test': v1_dl.get('test',{})}),\n",
                "    'CNN-BiLSTM V2': {'val': dl_v2_val, 'test': dl_v2_test}\n",
                "}\n",
                "\n",
                "print(f\"\\n{'='*75}\")\n",
                "print(f\"   FULL MODEL COMPARISON (including V2)\")\n",
                "print(f\"{'='*75}\")\n",
                "for split_name in ['val', 'test']:\n",
                "    print(f\"\\n--- {split_name.upper()} SET ---\")\n",
                "    print(f\"{'Model':<20} {'MAE':>8} {'RMSE':>8} {'MAPE%':>8} {'PeakMAE':>10}\")\n",
                "    print(\"-\"*60)\n",
                "    for model_name, m in all_metrics.items():\n",
                "        if split_name in m and m[split_name]:\n",
                "            t = m[split_name]\n",
                "            print(f\"{model_name:<20} {t['MAE']:>8} {t['RMSE']:>8} {t['MAPE']:>8} {t['Peak_MAE']:>10}\")\n",
                "\n",
                "fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
                "metric_names = ['MAE', 'RMSE', 'MAPE', 'Peak_MAE']\n",
                "model_names  = [m for m in all_metrics if 'test' in all_metrics[m] and all_metrics[m]['test']]\n",
                "colors = ['#e74c3c', '#3498db', '#95a5a6', '#2ecc71']\n",
                "\n",
                "for ax, metric in zip(axes, metric_names):\n",
                "    vals = [all_metrics[m]['test'][metric] for m in model_names]\n",
                "    bars = ax.bar(model_names, vals, color=colors[:len(model_names)], edgecolor='white', linewidth=1.5)\n",
                "    ax.set_title(metric, fontsize=13, fontweight='bold'); ax.set_ylabel(metric)\n",
                "    for bar, v in zip(bars, vals):\n",
                "        ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.5, f'{v}', ha='center', fontsize=9)\n",
                "    ax.grid(True, alpha=0.2, axis='y'); ax.tick_params(axis='x', rotation=15)\n",
                "plt.suptitle('Model Comparison â€” Test Set (V1 vs V2)', fontsize=15, fontweight='bold', y=1.03)\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(V2_DIR, 'model_comparison_v2.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Improvement over XGBoost\n",
                "xgb_test = baseline_metrics['XGBoost']['test']\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"   V2 IMPROVEMENT vs XGBoost\")\n",
                "print(f\"{'='*50}\")\n",
                "for metric in ['MAE', 'RMSE', 'MAPE', 'Peak_MAE']:\n",
                "    xgb_v = xgb_test[metric]\n",
                "    v2_v  = dl_v2_test[metric]\n",
                "    pct = (xgb_v - v2_v) / xgb_v * 100\n",
                "    arrow = 'âœ…â†“' if pct > 0 else 'âŒâ†‘'\n",
                "    print(f\"  {metric:10s}: XGBoost={xgb_v:>8} â†’ V2={v2_v:>8}  ({pct:+.1f}%) {arrow}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Save everything\n",
                "dl_v2_full = {\n",
                "    'val': dl_v2_val, 'test': dl_v2_test,\n",
                "    'horizon_wise': {str(k): v for k, v in horizon_metrics.items()},\n",
                "    'rolling_origin': {'per_origin': rolling_metrics, 'average': avg_rolling},\n",
                "    'hyperparameters': HP,\n",
                "    'total_params': int(total_params),\n",
                "    'best_val_loss': float(best_val_loss)\n",
                "}\n",
                "with open(os.path.join(V2_DIR, 'dl_metrics_v2.json'), 'w') as f:\n",
                "    json.dump(dl_v2_full, f, indent=2, cls=NumpyEncoder)\n",
                "\n",
                "import pandas as pd\n",
                "pd.DataFrame(history).to_csv(os.path.join(V2_DIR, 'dl_training_history_v2.csv'), index=False)\n",
                "\n",
                "comparison_save = {k: v for k, v in all_metrics.items() if 'test' in v and v['test']}\n",
                "with open(os.path.join(V2_DIR, 'all_model_comparison_v2.json'), 'w') as f:\n",
                "    json.dump(comparison_save, f, indent=2, cls=NumpyEncoder)\n",
                "\n",
                "print(\"\\nâœ… All V2 artifacts saved to models/v2/\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
                "n_test = len(y_pred_test)\n",
                "for idx, ax in enumerate(axes.flat):\n",
                "    si = idx * (n_test // 4)\n",
                "    ax.plot(inverse_scale(y_test_w[si]), 'k-', lw=2, label='Actual')\n",
                "    ax.plot(inverse_scale(y_pred_test[si]), 'g--', lw=1.5, label='V2 Prediction')\n",
                "    ax.set_title(f'Test Window #{si}'); ax.set_xlabel('Hour'); ax.set_ylabel('Load (MW)')\n",
                "    ax.legend(); ax.grid(True, alpha=0.3)\n",
                "plt.suptitle('CNN-BiLSTM-Attention V2 â€” Predictions vs Actual', fontsize=14, y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(V2_DIR, 'dl_sample_predictions_v2.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"ğŸ‰ V2 complete!\")"
            ],
            "execution_count": null,
            "outputs": []
        }
    ]
}
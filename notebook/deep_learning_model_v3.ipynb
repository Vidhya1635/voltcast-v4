{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Deep Learning Model V3 â€” Root Cause Fixes\n",
    "### Changes from V2:\n",
    "1. **5x smaller model** (~800K vs 3.8M params) â€” directly combats overfitting\n",
    "2. **Asymmetric loss** â€” penalizes under-prediction 1.5x more than over-prediction\n",
    "3. **Deep Ensemble** â€” 3 models with different seeds, averaged predictions\n",
    "4. **SWA** â€” Stochastic Weight Averaging for better generalization\n",
    "5. **Bias correction** â€” post-processing calibration from validation set\n",
    "6. **Scaling augmentation** â€” random magnitude warping during training\n",
    "7. **OneCycleLR** â€” more aggressive learning rate schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, json, time, math, copy, numpy as np, joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.integer,)): return int(obj)\n",
    "        if isinstance(obj, (np.floating,)): return float(obj)\n",
    "        if isinstance(obj, np.ndarray): return obj.tolist()\n",
    "        return super().default(obj)\n",
    "\n",
    "BASE_DIR  = '/content/drive/MyDrive/Electricity_Load_Forecast'\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
    "V3_DIR    = os.path.join(MODEL_DIR, 'v3')\n",
    "os.makedirs(V3_DIR, exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "config        = joblib.load(os.path.join(MODEL_DIR, 'config.pkl'))\n",
    "target_scaler = joblib.load(os.path.join(MODEL_DIR, 'target_scaler.pkl'))\n",
    "X_train_w = np.load(os.path.join(MODEL_DIR, 'X_train_w.npy'))\n",
    "y_train_w = np.load(os.path.join(MODEL_DIR, 'y_train_w.npy'))\n",
    "X_val_w   = np.load(os.path.join(MODEL_DIR, 'X_val_w.npy'))\n",
    "y_val_w   = np.load(os.path.join(MODEL_DIR, 'y_val_w.npy'))\n",
    "X_test_w  = np.load(os.path.join(MODEL_DIR, 'X_test_w.npy'))\n",
    "y_test_w  = np.load(os.path.join(MODEL_DIR, 'y_test_w.npy'))\n",
    "N_FEATURES = config['N_FEATURES']\n",
    "INPUT_LEN  = config['INPUT_LEN']\n",
    "OUTPUT_LEN = config['OUTPUT_LEN']\n",
    "print(f'Features={N_FEATURES}, In={INPUT_LEN}h, Out={OUTPUT_LEN}h')\n",
    "print(f'Train={X_train_w.shape}, Val={X_val_w.shape}, Test={X_test_w.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "HP = {\n",
    "    'batch_size': 64, 'lr': 5e-4, 'weight_decay': 5e-3,\n",
    "    'max_epochs': 60, 'patience': 15, 'grad_clip': 0.5,\n",
    "    'dropout': 0.35, 'noise_std': 0.03,\n",
    "    'mixup_alpha': 0.3, 'scale_aug': 0.05,\n",
    "    'conv_filters': 48, 'lstm_hidden': 128, 'n_heads': 4,\n",
    "    'huber_delta': 1.0, 'under_weight': 1.5,\n",
    "    'peak_weight': 2.5, 'peak_fraction': 0.2,\n",
    "    'swa_start_frac': 0.7, 'swa_lr': 1e-5,\n",
    "    'n_ensemble': 3, 'seeds': [42, 123, 456],\n",
    "}\n",
    "print('Hyperparameters:', json.dumps(HP, indent=2))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def make_loader(X, y, bs, shuffle=True):\n",
    "    ds = TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
    "                       torch.tensor(y, dtype=torch.float32))\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle,\n",
    "                      num_workers=2, pin_memory=True, drop_last=shuffle)\n",
    "\n",
    "train_loader = make_loader(X_train_w, y_train_w, HP['batch_size'])\n",
    "val_loader   = make_loader(X_val_w, y_val_w, HP['batch_size'], shuffle=False)\n",
    "test_loader  = make_loader(X_test_w, y_test_w, HP['batch_size'], shuffle=False)\n",
    "print(f'Batches â€” Train:{len(train_loader)}, Val:{len(val_loader)}, Test:{len(test_loader)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  V3 ARCHITECTURE â€” Lighter & better\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "class LightCNN(nn.Module):\n",
    "    \"\"\"Two-branch CNN (k=3, k=7) instead of three-branch.\"\"\"\n",
    "    def __init__(self, ch, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.conv3 = nn.Conv1d(ch, ch, 3, padding=1)\n",
    "        self.conv7 = nn.Conv1d(ch, ch, 7, padding=3)\n",
    "        self.bn    = nn.BatchNorm1d(ch * 2)\n",
    "        self.proj  = nn.Conv1d(ch * 2, ch, 1)\n",
    "        self.bn2   = nn.BatchNorm1d(ch)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c3 = F.gelu(self.conv3(x))\n",
    "        c7 = F.gelu(self.conv7(x))\n",
    "        out = F.gelu(self.bn(torch.cat([c3, c7], dim=1)))\n",
    "        return self.drop(F.gelu(self.bn2(self.proj(out))))\n",
    "\n",
    "\n",
    "class LoadForecastV3(nn.Module):\n",
    "    \"\"\"Lighter model: per-timestep decoder avoids information bottleneck.\n",
    "    Input(B,168,F) â†’ CNN â†’ BiLSTM â†’ SelfAttn â†’ FFN â†’ Decoder(B,168,1) â†’ (B,168)\"\"\"\n",
    "    def __init__(self, n_features, seq_len, pred_len,\n",
    "                 conv_filters=48, lstm_hidden=128, n_heads=4,\n",
    "                 dropout=0.35, noise_std=0.03):\n",
    "        super().__init__()\n",
    "        self.noise_std = noise_std\n",
    "        d = lstm_hidden * 2\n",
    "\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(n_features, conv_filters),\n",
    "            nn.LayerNorm(conv_filters), nn.GELU(), nn.Dropout(dropout * 0.5))\n",
    "\n",
    "        self.cnn = LightCNN(conv_filters, dropout * 0.5)\n",
    "\n",
    "        self.lstm = nn.LSTM(conv_filters, lstm_hidden,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.ln1  = nn.LayerNorm(d)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.attn    = nn.MultiheadAttention(d, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2     = nn.LayerNorm(d)\n",
    "        self.ffn     = nn.Sequential(nn.Linear(d, d*2), nn.GELU(),\n",
    "                                     nn.Dropout(dropout), nn.Linear(d*2, d))\n",
    "        self.ln3     = nn.LayerNorm(d)\n",
    "\n",
    "        # Per-timestep decoder: each LSTM position â†’ 1 output value\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, d // 2), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(d // 2, 1))\n",
    "        self.output_bias = nn.Parameter(torch.zeros(pred_len))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.noise_std > 0:\n",
    "            x = x + torch.randn_like(x) * self.noise_std\n",
    "        x = self.input_proj(x)                    # (B,168,48)\n",
    "        x = self.cnn(x.permute(0,2,1)).permute(0,2,1)  # CNN\n",
    "        h, _ = self.lstm(x)                       # (B,168,256)\n",
    "        h = self.drop(self.ln1(h))\n",
    "        a, _ = self.attn(h, h, h)\n",
    "        h = self.ln2(h + a)\n",
    "        h = self.ln3(h + self.ffn(h))\n",
    "        return self.decoder(h).squeeze(-1) + self.output_bias  # (B,168)\n",
    "\n",
    "# Quick param count\n",
    "_m = LoadForecastV3(N_FEATURES, INPUT_LEN, OUTPUT_LEN,\n",
    "                    HP['conv_filters'], HP['lstm_hidden'], HP['n_heads'],\n",
    "                    HP['dropout'], HP['noise_std'])\n",
    "print(f'V3 params: {sum(p.numel() for p in _m.parameters()):,}')\n",
    "del _m"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  ASYMMETRIC PEAK-AWARE LOSS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "class AsymmetricPeakLoss(nn.Module):\n",
    "    \"\"\"Huber + asymmetric penalty (under-prediction costs more) + peak weighting.\"\"\"\n",
    "    def __init__(self, delta=1.0, under_weight=1.5, peak_weight=2.5, peak_fraction=0.2):\n",
    "        super().__init__()\n",
    "        self.huber = nn.HuberLoss(delta=delta, reduction='none')\n",
    "        self.uw = under_weight\n",
    "        self.pw = peak_weight\n",
    "        self.pf = peak_fraction\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        loss = self.huber(pred, target)\n",
    "        # Asymmetric: under-prediction penalised more\n",
    "        aw = 1.0 + (self.uw - 1.0) * (pred < target).float()\n",
    "        # Peak weighting\n",
    "        k  = max(1, int(target.shape[1] * self.pf))\n",
    "        th = torch.topk(target, k, dim=1).values[:, -1:].detach()\n",
    "        pw = 1.0 + (self.pw - 1.0) * (target >= th).float()\n",
    "        return (loss * aw * pw).mean()\n",
    "\n",
    "\n",
    "def augment_batch(X, y, alpha=0.3, scale_range=0.05):\n",
    "    \"\"\"Mixup + random magnitude scaling.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = max(np.random.beta(alpha, alpha), 0.5)\n",
    "        idx = torch.randperm(X.size(0), device=X.device)\n",
    "        X = lam * X + (1-lam) * X[idx]\n",
    "        y = lam * y + (1-lam) * y[idx]\n",
    "    if scale_range > 0:\n",
    "        s = 1.0 + (torch.rand(X.size(0),1,1, device=X.device)*2-1) * scale_range\n",
    "        X = X * s\n",
    "        y = y * s.squeeze(-1)\n",
    "    return X, y\n",
    "\n",
    "print('âœ… Loss & augmentation defined')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  TRAIN ONE MODEL (with SWA)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def train_one(seed, HP, train_loader, val_loader, device, V3_DIR, idx):\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed(seed)\n",
    "\n",
    "    model = LoadForecastV3(\n",
    "        N_FEATURES, INPUT_LEN, OUTPUT_LEN,\n",
    "        HP['conv_filters'], HP['lstm_hidden'], HP['n_heads'],\n",
    "        HP['dropout'], HP['noise_std']\n",
    "    ).to(device)\n",
    "\n",
    "    crit = AsymmetricPeakLoss(HP['huber_delta'], HP['under_weight'],\n",
    "                              HP['peak_weight'], HP['peak_fraction'])\n",
    "    opt  = torch.optim.AdamW(model.parameters(), lr=HP['lr'], weight_decay=HP['weight_decay'])\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        opt, max_lr=HP['lr'], epochs=HP['max_epochs'],\n",
    "        steps_per_epoch=len(train_loader), pct_start=0.1,\n",
    "        anneal_strategy='cos', div_factor=10, final_div_factor=100)\n",
    "\n",
    "    swa_start = int(HP['max_epochs'] * HP['swa_start_frac'])\n",
    "    swa_model = AveragedModel(model)\n",
    "\n",
    "    hist = {'train': [], 'val': [], 'lr': []}\n",
    "    best_vl, patience_ctr, best_state = float('inf'), 0, None\n",
    "\n",
    "    print(f\"\\n{'â”€'*45}\")\n",
    "    print(f\"  Model {idx+1}/{HP['n_ensemble']}  seed={seed}\")\n",
    "    print(f\"{'â”€'*45}\")\n",
    "\n",
    "    for ep in range(HP['max_epochs']):\n",
    "        t0 = time.time()\n",
    "        model.train(); losses = []\n",
    "        for X_b, y_b in train_loader:\n",
    "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "            X_b, y_b = augment_batch(X_b, y_b, HP['mixup_alpha'], HP['scale_aug'])\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(X_b), y_b)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), HP['grad_clip'])\n",
    "            opt.step(); sched.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if ep >= swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "\n",
    "        model.eval(); vl = []\n",
    "        with torch.no_grad():\n",
    "            for X_b, y_b in val_loader:\n",
    "                vl.append(crit(model(X_b.to(device)), y_b.to(device)).item())\n",
    "\n",
    "        tl = float(np.mean(losses)); vl = float(np.mean(vl))\n",
    "        lr = float(opt.param_groups[0]['lr'])\n",
    "        hist['train'].append(tl); hist['val'].append(vl); hist['lr'].append(lr)\n",
    "\n",
    "        tag = ''\n",
    "        if vl < best_vl:\n",
    "            best_vl, patience_ctr = vl, 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            tag = ' âœ…'\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "\n",
    "        if (ep+1) % 5 == 0 or tag:\n",
    "            print(f'  Ep {ep+1:3d}/{HP[\"max_epochs\"]} | '\n",
    "                  f'T:{tl:.4f} V:{vl:.4f} LR:{lr:.1e} {time.time()-t0:.1f}s{tag}')\n",
    "\n",
    "        if patience_ctr >= HP['patience']:\n",
    "            print(f'  â›” Early stop ep {ep+1}'); break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    swa_model.update_parameters(model)\n",
    "    try: update_bn(train_loader, swa_model, device=device)\n",
    "    except: pass\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(V3_DIR, f'model_{idx}_s{seed}.pt'))\n",
    "    print(f'  âœ… Done (best val={best_vl:.4f})')\n",
    "    return model, swa_model, hist, best_vl"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  TRAIN ENSEMBLE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "models, swa_models, histories, val_losses = [], [], [], []\n",
    "for i, seed in enumerate(HP['seeds'][:HP['n_ensemble']]):\n",
    "    m, sm, h, vl = train_one(seed, HP, train_loader, val_loader, device, V3_DIR, i)\n",
    "    models.append(m); swa_models.append(sm)\n",
    "    histories.append(h); val_losses.append(vl)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f'  ENSEMBLE DONE â€” val losses: {[round(v,4) for v in val_losses]}')\n",
    "print(f'  Average val loss: {np.mean(val_losses):.4f}')\n",
    "print(f\"{'='*50}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, HP['n_ensemble'], figsize=(6*HP['n_ensemble'], 5))\n",
    "if HP['n_ensemble'] == 1: axes = [axes]\n",
    "for i, (h, ax) in enumerate(zip(histories, axes)):\n",
    "    ax.plot(h['train'], label='Train', color='#2196F3')\n",
    "    ax.plot(h['val'],   label='Val',   color='#FF5722')\n",
    "    ax.set_title(f'Model {i+1} (seed={HP[\"seeds\"][i]})')\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
    "    ax.legend(); ax.grid(True, alpha=0.3)\n",
    "plt.suptitle('V3 Ensemble Training', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(V3_DIR, 'training_history_v3.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  EVALUATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def predict_all(model, loader):\n",
    "    model.eval(); ps = []\n",
    "    with torch.no_grad():\n",
    "        for X_b, _ in loader:\n",
    "            ps.append(model(X_b.to(device)).cpu().numpy())\n",
    "    return np.concatenate(ps)\n",
    "\n",
    "def ensemble_predict(models, loader):\n",
    "    return np.mean([predict_all(m, loader) for m in models], axis=0)\n",
    "\n",
    "def inv(y): return target_scaler.inverse_transform(y.reshape(-1,1)).reshape(y.shape)\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    yt, yp = inv(y_true), inv(y_pred)\n",
    "    mae  = float(mean_absolute_error(yt.flat, yp.flat))\n",
    "    rmse = float(np.sqrt(mean_squared_error(yt.flat, yp.flat)))\n",
    "    mask = yt.flat[:] != 0\n",
    "    mape = float(np.mean(np.abs((yt.flat[mask]-yp.flat[mask])/yt.flat[mask]))*100)\n",
    "    pk   = [float(np.abs(yt[i,np.argmax(yt[i])]-yp[i,np.argmax(yt[i])])) for i in range(len(yt))]\n",
    "    return {'MAE':round(mae,2),'RMSE':round(rmse,2),'MAPE':round(mape,2),'Peak_MAE':round(float(np.mean(pk)),2)}\n",
    "\n",
    "# Individual model results\n",
    "print('Individual models (test):')\n",
    "for i, m in enumerate(models):\n",
    "    p = predict_all(m, test_loader)\n",
    "    print(f'  Model {i+1}: {metrics(y_test_w[:len(p)], p)}')\n",
    "\n",
    "# Ensemble\n",
    "yp_val_ens  = ensemble_predict(models, val_loader)\n",
    "yp_test_ens = ensemble_predict(models, test_loader)\n",
    "ens_val  = metrics(y_val_w[:len(yp_val_ens)],  yp_val_ens)\n",
    "ens_test = metrics(y_test_w[:len(yp_test_ens)], yp_test_ens)\n",
    "\n",
    "print(f\"\\n{'='*40}\\nEnsemble â€” Val:  {ens_val}\")\n",
    "print(f\"Ensemble â€” Test: {ens_test}\\n{'='*40}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  BIAS CORRECTION (from validation set)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "vp_inv = inv(yp_val_ens)\n",
    "vt_inv = inv(y_val_w[:len(yp_val_ens)])\n",
    "bias_per_hour = np.mean(vt_inv - vp_inv, axis=0)  # (168,)\n",
    "print(f'Mean additive bias: {np.mean(bias_per_hour):.1f} MW')\n",
    "\n",
    "# Apply to test\n",
    "tp_inv = inv(yp_test_ens)\n",
    "tt_inv = inv(y_test_w[:len(yp_test_ens)])\n",
    "tp_corr = tp_inv + bias_per_hour\n",
    "\n",
    "mae_c  = float(mean_absolute_error(tt_inv.flat, tp_corr.flat))\n",
    "rmse_c = float(np.sqrt(mean_squared_error(tt_inv.flat, tp_corr.flat)))\n",
    "mask   = tt_inv.flat[:] != 0\n",
    "mape_c = float(np.mean(np.abs((tt_inv.flat[mask]-tp_corr.flat[mask])/tt_inv.flat[mask]))*100)\n",
    "pk_c   = [float(np.abs(tt_inv[i,np.argmax(tt_inv[i])]-tp_corr[i,np.argmax(tt_inv[i])])) for i in range(len(tt_inv))]\n",
    "corr_test = {'MAE':round(mae_c,2),'RMSE':round(rmse_c,2),'MAPE':round(mape_c,2),'Peak_MAE':round(float(np.mean(pk_c)),2)}\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBias-Corrected Ensemble â€” Test: {corr_test}\\n{'='*40}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  HORIZON-WISE ERROR\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "horizons = [1, 24, 72, 168]\n",
    "hmets = {}\n",
    "for h in horizons:\n",
    "    i = h - 1\n",
    "    m_h = float(mean_absolute_error(tt_inv[:,i], tp_corr[:,i]))\n",
    "    r_h = float(np.sqrt(mean_squared_error(tt_inv[:,i], tp_corr[:,i])))\n",
    "    mk  = tt_inv[:,i] != 0\n",
    "    mp_h = float(np.mean(np.abs((tt_inv[:,i][mk]-tp_corr[:,i][mk])/tt_inv[:,i][mk]))*100)\n",
    "    hmets[h] = {'MAE':round(m_h,2),'RMSE':round(r_h,2),'MAPE':round(mp_h,2)}\n",
    "print(f\"{'Hour':>6} {'MAE':>10} {'RMSE':>10} {'MAPE%':>10}\")\n",
    "for h,m in hmets.items(): print(f'{h:>6} {m[\"MAE\"]:>10} {m[\"RMSE\"]:>10} {m[\"MAPE\"]:>10}')\n",
    "\n",
    "mph = [float(mean_absolute_error(tt_inv[:,h], tp_corr[:,h])) for h in range(OUTPUT_LEN)]\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(range(1,OUTPUT_LEN+1), mph, 'b-', lw=1.5)\n",
    "for h in horizons:\n",
    "    plt.axvline(h, color='red', ls='--', alpha=0.5)\n",
    "    plt.annotate(f'h={h}', (h, mph[h-1]), fontsize=9, color='red')\n",
    "plt.xlabel('Horizon (h)'); plt.ylabel('MAE (MW)')\n",
    "plt.title('Horizon-Wise MAE â€” V3 Corrected'); plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(V3_DIR, 'horizon_wise_v3.png'), dpi=150)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  FULL MODEL COMPARISON\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "with open(os.path.join(MODEL_DIR, 'baseline_metrics.json')) as f:\n",
    "    bl = json.load(f)\n",
    "v1, v2 = {}, {}\n",
    "try:\n",
    "    with open(os.path.join(MODEL_DIR, 'dl_metrics.json')) as f:\n",
    "        raw = json.load(f)\n",
    "    v1 = raw.get('CNN-BiLSTM-Attn', {'val':raw.get('val',{}),'test':raw.get('test',{})})\n",
    "except: pass\n",
    "try:\n",
    "    with open(os.path.join(MODEL_DIR,'v2','dl_metrics_v2.json')) as f:\n",
    "        raw2 = json.load(f)\n",
    "    v2 = {'val':raw2.get('val',{}),'test':raw2.get('test',{})}\n",
    "except: pass\n",
    "\n",
    "all_m = {'Persistence': bl['Persistence'], 'XGBoost': bl['XGBoost']}\n",
    "if v1: all_m['DL V1'] = v1\n",
    "if v2: all_m['DL V2'] = v2\n",
    "all_m['DL V3 Ensemble'] = {'val': ens_val, 'test': ens_test}\n",
    "all_m['DL V3 Corrected'] = {'test': corr_test}\n",
    "\n",
    "print(f\"\\n{'='*75}\")\n",
    "print(f'   FULL MODEL COMPARISON')\n",
    "print(f\"{'='*75}\")\n",
    "for sp in ['val','test']:\n",
    "    print(f\"\\n--- {sp.upper()} ---\")\n",
    "    print(f\"{'Model':<22} {'MAE':>8} {'RMSE':>8} {'MAPE%':>8} {'PeakMAE':>10}\")\n",
    "    print('-'*62)\n",
    "    for n,m in all_m.items():\n",
    "        if sp in m and m[sp]:\n",
    "            t=m[sp]\n",
    "            print(f\"{n:<22} {t['MAE']:>8} {t['RMSE']:>8} {t['MAPE']:>8} {t['Peak_MAE']:>10}\")\n",
    "\n",
    "# Improvement vs XGBoost\n",
    "xgb = bl['XGBoost']['test']\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f'   V3 IMPROVEMENT vs XGBoost')\n",
    "print(f\"{'='*55}\")\n",
    "for label, mt in [('V3 Ensemble', ens_test), ('V3 Corrected', corr_test)]:\n",
    "    print(f'\\n  {label}:')\n",
    "    for k in ['MAE','RMSE','MAPE','Peak_MAE']:\n",
    "        xv, v3v = xgb[k], mt[k]\n",
    "        pct = (xv - v3v) / xv * 100\n",
    "        arr = 'âœ…â†“' if pct > 0 else 'âŒâ†‘'\n",
    "        print(f'    {k:10s}: XGB={xv:>8} â†’ V3={v3v:>8}  ({pct:+.1f}%) {arr}')\n",
    "\n",
    "# Bar chart\n",
    "fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "mns = [n for n in all_m if 'test' in all_m[n] and all_m[n]['test']]\n",
    "colors = ['#e74c3c','#3498db','#95a5a6','#f39c12','#2ecc71','#1abc9c']\n",
    "for ax, mk in zip(axes, ['MAE','RMSE','MAPE','Peak_MAE']):\n",
    "    vals = [all_m[n]['test'][mk] for n in mns]\n",
    "    bars = ax.bar(range(len(mns)), vals, color=colors[:len(mns)], edgecolor='white')\n",
    "    ax.set_title(mk, fontweight='bold'); ax.set_ylabel(mk)\n",
    "    ax.set_xticks(range(len(mns))); ax.set_xticklabels(mns, rotation=25, ha='right', fontsize=8)\n",
    "    for b,v in zip(bars,vals): ax.text(b.get_x()+b.get_width()/2, b.get_height()+0.5, f'{v}', ha='center', fontsize=8)\n",
    "    ax.grid(True, alpha=0.2, axis='y')\n",
    "plt.suptitle('All Models Comparison â€” Test Set', fontsize=14, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(V3_DIR, 'model_comparison_v3.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  SAVE ALL ARTIFACTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import pandas as pd\n",
    "v3_full = {\n",
    "    'ensemble_val': ens_val, 'ensemble_test': ens_test,\n",
    "    'corrected_test': corr_test,\n",
    "    'horizon_wise': {str(k):v for k,v in hmets.items()},\n",
    "    'individual_val_losses': [round(float(v),4) for v in val_losses],\n",
    "    'hyperparameters': HP,\n",
    "    'bias_per_hour_mean': round(float(np.mean(bias_per_hour)),2),\n",
    "}\n",
    "with open(os.path.join(V3_DIR, 'dl_metrics_v3.json'), 'w') as f:\n",
    "    json.dump(v3_full, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "for i,h in enumerate(histories):\n",
    "    pd.DataFrame(h).to_csv(os.path.join(V3_DIR, f'history_model{i}.csv'), index=False)\n",
    "\n",
    "comp_save = {k:v for k,v in all_m.items() if 'test' in v and v['test']}\n",
    "with open(os.path.join(V3_DIR, 'all_model_comparison_v3.json'), 'w') as f:\n",
    "    json.dump(comp_save, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "np.save(os.path.join(V3_DIR, 'bias_per_hour.npy'), bias_per_hour)\n",
    "print('\\nâœ… All V3 artifacts saved to models/v3/')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sample predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "n = len(yp_test_ens)\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    si = idx * (n // 4)\n",
    "    ax.plot(tt_inv[si], 'k-', lw=2, label='Actual')\n",
    "    ax.plot(tp_corr[si], 'g--', lw=1.5, label='V3 Corrected')\n",
    "    ax.plot(tp_inv[si], 'b:', lw=1, alpha=0.5, label='V3 Raw')\n",
    "    ax.set_title(f'Test #{si}'); ax.set_xlabel('Hour'); ax.set_ylabel('MW')\n",
    "    ax.legend(); ax.grid(True, alpha=0.3)\n",
    "plt.suptitle('V3 Predictions (Corrected vs Raw vs Actual)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(V3_DIR, 'sample_predictions_v3.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('ğŸ‰ V3 complete!')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}

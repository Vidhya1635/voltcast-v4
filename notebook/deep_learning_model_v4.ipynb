{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ§  V4 â€” Hybrid Residual Learning (XGBoost + DL)\n",
                "### Strategy: DL learns what XGBoost gets wrong\n",
                "1. Generate XGBoost predictions for all windows\n",
                "2. Compute residuals: `residual = actual - XGBoost_pred`\n",
                "3. Train DL model to predict these residuals from input sequences\n",
                "4. Final prediction = `XGBoost_pred + DL_residual_pred`\n",
                "\n",
                "**Why this works:**\n",
                "- XGBoost already captures 85%+ of the signal (MAE=2120)\n",
                "- DL only needs to learn the remaining ~15% (temporal patterns XGBoost misses)\n",
                "- Residuals are small & centered around 0 â†’ much easier to learn\n",
                "- No under-prediction problem (residuals are symmetric)\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import os, json, time, math, copy, numpy as np, joblib\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "import warnings; warnings.filterwarnings('ignore')\n",
                "\n",
                "class NumpyEncoder(json.JSONEncoder):\n",
                "    def default(self, obj):\n",
                "        if isinstance(obj, (np.integer,)): return int(obj)\n",
                "        if isinstance(obj, (np.floating,)): return float(obj)\n",
                "        if isinstance(obj, np.ndarray): return obj.tolist()\n",
                "        return super().default(obj)\n",
                "\n",
                "BASE_DIR  = '/content/drive/MyDrive/Electricity_Load_Forecast'\n",
                "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
                "V4_DIR    = os.path.join(MODEL_DIR, 'v4')\n",
                "os.makedirs(V4_DIR, exist_ok=True)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device: {device}')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "config        = joblib.load(os.path.join(MODEL_DIR, 'config.pkl'))\n",
                "target_scaler = joblib.load(os.path.join(MODEL_DIR, 'target_scaler.pkl'))\n",
                "X_train_w = np.load(os.path.join(MODEL_DIR, 'X_train_w.npy'))\n",
                "y_train_w = np.load(os.path.join(MODEL_DIR, 'y_train_w.npy'))\n",
                "X_val_w   = np.load(os.path.join(MODEL_DIR, 'X_val_w.npy'))\n",
                "y_val_w   = np.load(os.path.join(MODEL_DIR, 'y_val_w.npy'))\n",
                "X_test_w  = np.load(os.path.join(MODEL_DIR, 'X_test_w.npy'))\n",
                "y_test_w  = np.load(os.path.join(MODEL_DIR, 'y_test_w.npy'))\n",
                "LOAD_IDX   = config['load_col_idx']\n",
                "N_FEATURES = config['N_FEATURES']\n",
                "INPUT_LEN  = config['INPUT_LEN']\n",
                "OUTPUT_LEN = config['OUTPUT_LEN']\n",
                "print(f'Features={N_FEATURES}, In={INPUT_LEN}h, Out={OUTPUT_LEN}h, LoadIdx={LOAD_IDX}')\n",
                "print(f'Train={X_train_w.shape}, Val={X_val_w.shape}, Test={X_test_w.shape}')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  STEP 1: GET XGBOOST PREDICTIONS\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "def engineer_features(X_windows, load_idx):\n",
                "    \"\"\"Same feature extraction as baseline_model notebook.\"\"\"\n",
                "    feat_list = []\n",
                "    for w in X_windows:\n",
                "        f = []\n",
                "        load = w[:, load_idx]\n",
                "        f.extend([load.mean(), load.std(), load.min(), load.max(),\n",
                "                  load[-1], load[0], load[-1] - load[0],\n",
                "                  load[-24:].mean(), load[-48:].mean()])\n",
                "        for j in range(w.shape[1]):\n",
                "            if j != load_idx:\n",
                "                f.extend([w[:, j].mean(), w[-1, j]])\n",
                "        feat_list.append(f)\n",
                "    return np.array(feat_list, dtype=np.float32)\n",
                "\n",
                "# Load saved XGBoost model\n",
                "print('Loading XGBoost model...')\n",
                "xgb_model = joblib.load(os.path.join(MODEL_DIR, 'xgb_model.pkl'))\n",
                "\n",
                "# Extract features & predict\n",
                "print('Generating XGBoost predictions for all windows...')\n",
                "t0 = time.time()\n",
                "Xf_train = engineer_features(X_train_w, LOAD_IDX)\n",
                "Xf_val   = engineer_features(X_val_w,   LOAD_IDX)\n",
                "Xf_test  = engineer_features(X_test_w,  LOAD_IDX)\n",
                "\n",
                "xgb_pred_train = xgb_model.predict(Xf_train)  # (N_train, 168)\n",
                "xgb_pred_val   = xgb_model.predict(Xf_val)\n",
                "xgb_pred_test  = xgb_model.predict(Xf_test)\n",
                "print(f'Done in {time.time()-t0:.1f}s')\n",
                "print(f'XGBoost pred shapes: train={xgb_pred_train.shape}, val={xgb_pred_val.shape}, test={xgb_pred_test.shape}')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  STEP 2: COMPUTE RESIDUALS\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# residual = actual - xgboost_prediction (in scaled space)\n",
                "res_train = y_train_w - xgb_pred_train\n",
                "res_val   = y_val_w   - xgb_pred_val\n",
                "res_test  = y_test_w  - xgb_pred_test\n",
                "\n",
                "print(f'Residual stats (scaled):')\n",
                "print(f'  Train â€” mean: {res_train.mean():.4f}, std: {res_train.std():.4f}')\n",
                "print(f'  Val   â€” mean: {res_val.mean():.4f}, std: {res_val.std():.4f}')\n",
                "print(f'  Test  â€” mean: {res_test.mean():.4f}, std: {res_test.std():.4f}')\n",
                "\n",
                "# Visualize residual distribution\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "for ax, data, title in zip(axes, [res_train, res_val, res_test], ['Train','Val','Test']):\n",
                "    ax.hist(data.flatten(), bins=100, alpha=0.7, color='steelblue')\n",
                "    ax.axvline(0, color='red', ls='--')\n",
                "    ax.set_title(f'{title} Residuals'); ax.set_xlabel('Residual (scaled)')\n",
                "plt.suptitle('XGBoost Residual Distribution (what DL needs to learn)', fontsize=13, y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(V4_DIR, 'residual_distribution.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  STEP 3: PREPARE DATA FOR DL\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "HP = {\n",
                "    'batch_size': 64, 'lr': 3e-4, 'weight_decay': 1e-3,\n",
                "    'max_epochs': 80, 'patience': 20, 'grad_clip': 0.5,\n",
                "    'dropout': 0.25, 'noise_std': 0.015,\n",
                "    'mixup_alpha': 0.2,\n",
                "    'conv_filters': 48, 'lstm_hidden': 128, 'n_heads': 4,\n",
                "    'n_ensemble': 3, 'seeds': [42, 123, 456],\n",
                "}\n",
                "print('HP:', json.dumps(HP, indent=2))\n",
                "\n",
                "# Include XGBoost prediction as extra input feature\n",
                "# Shape: (N, 168, N_FEATURES + 1) â€” original features + XGBoost pred\n",
                "def add_xgb_feature(X, xgb_pred):\n",
                "    return np.concatenate([X, xgb_pred[:, :, None]], axis=-1).astype(np.float32)\n",
                "\n",
                "X_train_aug = add_xgb_feature(X_train_w, xgb_pred_train)\n",
                "X_val_aug   = add_xgb_feature(X_val_w,   xgb_pred_val)\n",
                "X_test_aug  = add_xgb_feature(X_test_w,  xgb_pred_test)\n",
                "N_FEAT_AUG  = X_train_aug.shape[-1]\n",
                "print(f'Augmented input shape: {X_train_aug.shape} (added XGBoost pred as feature #{N_FEAT_AUG})')\n",
                "\n",
                "def make_loader(X, y, bs, shuffle=True):\n",
                "    ds = TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
                "                       torch.tensor(y, dtype=torch.float32))\n",
                "    return DataLoader(ds, batch_size=bs, shuffle=shuffle,\n",
                "                      num_workers=2, pin_memory=True, drop_last=shuffle)\n",
                "\n",
                "# DL targets are RESIDUALS, not raw values\n",
                "train_loader = make_loader(X_train_aug, res_train, HP['batch_size'])\n",
                "val_loader   = make_loader(X_val_aug,   res_val,   HP['batch_size'], shuffle=False)\n",
                "test_loader  = make_loader(X_test_aug,  res_test,  HP['batch_size'], shuffle=False)\n",
                "print(f'Batches â€” Train:{len(train_loader)}, Val:{len(val_loader)}, Test:{len(test_loader)}')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  STEP 4: RESIDUAL DL MODEL\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class LightCNN(nn.Module):\n",
                "    def __init__(self, ch, dropout=0.15):\n",
                "        super().__init__()\n",
                "        self.conv3 = nn.Conv1d(ch, ch, 3, padding=1)\n",
                "        self.conv7 = nn.Conv1d(ch, ch, 7, padding=3)\n",
                "        self.bn = nn.BatchNorm1d(ch * 2)\n",
                "        self.proj = nn.Conv1d(ch * 2, ch, 1)\n",
                "        self.bn2 = nn.BatchNorm1d(ch)\n",
                "        self.drop = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        c3 = F.gelu(self.conv3(x))\n",
                "        c7 = F.gelu(self.conv7(x))\n",
                "        out = F.gelu(self.bn(torch.cat([c3, c7], dim=1)))\n",
                "        return self.drop(F.gelu(self.bn2(self.proj(out))))\n",
                "\n",
                "\n",
                "class ResidualPredictor(nn.Module):\n",
                "    \"\"\"Predicts XGBoost residuals from enriched input sequences.\n",
                "    Input: (B, 168, N_FEATURES+1) where last feature = XGBoost prediction.\n",
                "    Output: (B, 168) predicted residual.\"\"\"\n",
                "    def __init__(self, n_features, pred_len,\n",
                "                 conv_filters=48, lstm_hidden=128, n_heads=4,\n",
                "                 dropout=0.25, noise_std=0.015):\n",
                "        super().__init__()\n",
                "        self.noise_std = noise_std\n",
                "        d = lstm_hidden * 2\n",
                "\n",
                "        self.input_proj = nn.Sequential(\n",
                "            nn.Linear(n_features, conv_filters),\n",
                "            nn.LayerNorm(conv_filters), nn.GELU(),\n",
                "            nn.Dropout(dropout * 0.5))\n",
                "\n",
                "        self.cnn = LightCNN(conv_filters, dropout * 0.5)\n",
                "\n",
                "        self.lstm = nn.LSTM(conv_filters, lstm_hidden,\n",
                "                            batch_first=True, bidirectional=True)\n",
                "        self.ln1 = nn.LayerNorm(d)\n",
                "        self.drop = nn.Dropout(dropout)\n",
                "\n",
                "        self.attn = nn.MultiheadAttention(d, n_heads, dropout=dropout, batch_first=True)\n",
                "        self.ln2 = nn.LayerNorm(d)\n",
                "        self.ffn = nn.Sequential(nn.Linear(d, d*2), nn.GELU(),\n",
                "                                 nn.Dropout(dropout), nn.Linear(d*2, d))\n",
                "        self.ln3 = nn.LayerNorm(d)\n",
                "\n",
                "        # Per-timestep output â†’ predicted residual\n",
                "        self.head = nn.Sequential(\n",
                "            nn.Linear(d, d // 2), nn.GELU(),\n",
                "            nn.Dropout(dropout), nn.Linear(d // 2, 1))\n",
                "\n",
                "    def forward(self, x):\n",
                "        if self.training and self.noise_std > 0:\n",
                "            x = x + torch.randn_like(x) * self.noise_std\n",
                "        x = self.input_proj(x)\n",
                "        x = self.cnn(x.permute(0,2,1)).permute(0,2,1)\n",
                "        h, _ = self.lstm(x)\n",
                "        h = self.drop(self.ln1(h))\n",
                "        a, _ = self.attn(h, h, h)\n",
                "        h = self.ln2(h + a)\n",
                "        h = self.ln3(h + self.ffn(h))\n",
                "        return self.head(h).squeeze(-1)  # (B, 168)\n",
                "\n",
                "_m = ResidualPredictor(N_FEAT_AUG, OUTPUT_LEN, HP['conv_filters'],\n",
                "                       HP['lstm_hidden'], HP['n_heads'], HP['dropout'])\n",
                "print(f'ResidualPredictor params: {sum(p.numel() for p in _m.parameters()):,}')\n",
                "del _m"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  STEP 5: TRAINING FUNCTION\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "def mixup(X, y, alpha=0.2):\n",
                "    if alpha <= 0: return X, y\n",
                "    lam = max(np.random.beta(alpha, alpha), 0.5)\n",
                "    idx = torch.randperm(X.size(0), device=X.device)\n",
                "    return lam*X + (1-lam)*X[idx], lam*y + (1-lam)*y[idx]\n",
                "\n",
                "def train_residual_model(seed, HP, train_loader, val_loader, device, save_dir, idx):\n",
                "    torch.manual_seed(seed); np.random.seed(seed)\n",
                "    if torch.cuda.is_available(): torch.cuda.manual_seed(seed)\n",
                "\n",
                "    model = ResidualPredictor(\n",
                "        N_FEAT_AUG, OUTPUT_LEN, HP['conv_filters'],\n",
                "        HP['lstm_hidden'], HP['n_heads'], HP['dropout'], HP['noise_std']\n",
                "    ).to(device)\n",
                "\n",
                "    # MSE loss for residuals (they're centered around 0, symmetric)\n",
                "    criterion = nn.HuberLoss(delta=0.5)\n",
                "    opt  = torch.optim.AdamW(model.parameters(), lr=HP['lr'],\n",
                "                             weight_decay=HP['weight_decay'])\n",
                "    sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
                "        opt, T_0=20, T_mult=2, eta_min=1e-6)\n",
                "\n",
                "    hist = {'train': [], 'val': [], 'lr': []}\n",
                "    best_vl, patience_ctr, best_state = float('inf'), 0, None\n",
                "\n",
                "    print(f\"\\n{'â”€'*45}\")\n",
                "    print(f\"  Model {idx+1}/{HP['n_ensemble']}  seed={seed}\")\n",
                "    print(f\"{'â”€'*45}\")\n",
                "\n",
                "    for ep in range(HP['max_epochs']):\n",
                "        t0 = time.time()\n",
                "        model.train(); losses = []\n",
                "        for X_b, y_b in train_loader:\n",
                "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
                "            X_b, y_b = mixup(X_b, y_b, HP['mixup_alpha'])\n",
                "            opt.zero_grad()\n",
                "            loss = criterion(model(X_b), y_b)\n",
                "            loss.backward()\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), HP['grad_clip'])\n",
                "            opt.step()\n",
                "            losses.append(loss.item())\n",
                "        sched.step()\n",
                "\n",
                "        model.eval(); vl = []\n",
                "        with torch.no_grad():\n",
                "            for X_b, y_b in val_loader:\n",
                "                vl.append(criterion(model(X_b.to(device)), y_b.to(device)).item())\n",
                "\n",
                "        tl = float(np.mean(losses)); vl = float(np.mean(vl))\n",
                "        lr = float(opt.param_groups[0]['lr'])\n",
                "        hist['train'].append(tl); hist['val'].append(vl); hist['lr'].append(lr)\n",
                "\n",
                "        tag = ''\n",
                "        if vl < best_vl:\n",
                "            best_vl, patience_ctr = vl, 0\n",
                "            best_state = copy.deepcopy(model.state_dict())\n",
                "            tag = ' âœ…'\n",
                "        else:\n",
                "            patience_ctr += 1\n",
                "\n",
                "        if (ep+1) % 5 == 0 or tag:\n",
                "            print(f'  Ep {ep+1:3d}/{HP[\"max_epochs\"]} | '\n",
                "                  f'T:{tl:.5f} V:{vl:.5f} LR:{lr:.1e} {time.time()-t0:.1f}s{tag}')\n",
                "\n",
                "        if patience_ctr >= HP['patience']:\n",
                "            print(f'  â›” Early stop ep {ep+1}'); break\n",
                "\n",
                "    model.load_state_dict(best_state)\n",
                "    torch.save(model.state_dict(), os.path.join(save_dir, f'res_model_{idx}_s{seed}.pt'))\n",
                "    print(f'  âœ… Done (best val={best_vl:.5f})')\n",
                "    return model, hist, best_vl"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  STEP 6: TRAIN ENSEMBLE\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "models, histories, val_losses = [], [], []\n",
                "for i, seed in enumerate(HP['seeds'][:HP['n_ensemble']]):\n",
                "    m, h, vl = train_residual_model(seed, HP, train_loader, val_loader, device, V4_DIR, i)\n",
                "    models.append(m); histories.append(h); val_losses.append(vl)\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f'  ENSEMBLE DONE â€” val losses: {[round(v,5) for v in val_losses]}')\n",
                "print(f\"{'='*50}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Training history\n",
                "fig, axes = plt.subplots(1, HP['n_ensemble'], figsize=(6*HP['n_ensemble'], 5))\n",
                "if HP['n_ensemble'] == 1: axes = [axes]\n",
                "for i, (h, ax) in enumerate(zip(histories, axes)):\n",
                "    ax.plot(h['train'], label='Train', color='#2196F3')\n",
                "    ax.plot(h['val'],   label='Val',   color='#FF5722')\n",
                "    ax.set_title(f'Residual Model {i+1} (seed={HP[\"seeds\"][i]})')\n",
                "    ax.set_xlabel('Epoch'); ax.set_ylabel('Huber Loss')\n",
                "    ax.legend(); ax.grid(True, alpha=0.3)\n",
                "plt.suptitle('V4 Residual Models â€” Training History', fontsize=14, y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(V4_DIR, 'training_history_v4.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  STEP 7: COMBINE XGBoost + DL\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "def predict_residuals(model, loader):\n",
                "    model.eval(); ps = []\n",
                "    with torch.no_grad():\n",
                "        for X_b, _ in loader:\n",
                "            ps.append(model(X_b.to(device)).cpu().numpy())\n",
                "    return np.concatenate(ps)\n",
                "\n",
                "def ensemble_residuals(models, loader):\n",
                "    return np.mean([predict_residuals(m, loader) for m in models], axis=0)\n",
                "\n",
                "def inv(y):\n",
                "    return target_scaler.inverse_transform(y.reshape(-1,1)).reshape(y.shape)\n",
                "\n",
                "def compute_metrics(y_true_scaled, y_pred_scaled):\n",
                "    yt, yp = inv(y_true_scaled), inv(y_pred_scaled)\n",
                "    mae  = float(mean_absolute_error(yt.flat, yp.flat))\n",
                "    rmse = float(np.sqrt(mean_squared_error(yt.flat, yp.flat)))\n",
                "    mask = yt.flat[:] != 0\n",
                "    mape = float(np.mean(np.abs((yt.flat[mask]-yp.flat[mask])/yt.flat[mask]))*100)\n",
                "    pk = [float(np.abs(yt[i,np.argmax(yt[i])]-yp[i,np.argmax(yt[i])])) for i in range(len(yt))]\n",
                "    return {'MAE':round(mae,2),'RMSE':round(rmse,2),'MAPE':round(mape,2),'Peak_MAE':round(float(np.mean(pk)),2)}\n",
                "\n",
                "# Get predicted residuals\n",
                "res_pred_val  = ensemble_residuals(models, val_loader)\n",
                "res_pred_test = ensemble_residuals(models, test_loader)\n",
                "\n",
                "# HYBRID prediction = XGBoost + DL residual\n",
                "hybrid_pred_val  = xgb_pred_val[:len(res_pred_val)]   + res_pred_val\n",
                "hybrid_pred_test = xgb_pred_test[:len(res_pred_test)] + res_pred_test\n",
                "\n",
                "# Trim targets to match\n",
                "y_val_trim  = y_val_w[:len(res_pred_val)]\n",
                "y_test_trim = y_test_w[:len(res_pred_test)]\n",
                "\n",
                "# XGBoost-only metrics\n",
                "xgb_val_m  = compute_metrics(y_val_trim,  xgb_pred_val[:len(res_pred_val)])\n",
                "xgb_test_m = compute_metrics(y_test_trim, xgb_pred_test[:len(res_pred_test)])\n",
                "\n",
                "# Hybrid metrics\n",
                "hyb_val_m  = compute_metrics(y_val_trim,  hybrid_pred_val)\n",
                "hyb_test_m = compute_metrics(y_test_trim, hybrid_pred_test)\n",
                "\n",
                "print(f\"\\n{'='*50}\\nXGBoost Only â€” Val:  {xgb_val_m}\")\n",
                "print(f\"XGBoost Only â€” Test: {xgb_test_m}\")\n",
                "print(f\"\\nHybrid (XGB+DL) â€” Val:  {hyb_val_m}\")\n",
                "print(f\"Hybrid (XGB+DL) â€” Test: {hyb_test_m}\\n{'='*50}\")\n",
                "\n",
                "# Improvement\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f'   HYBRID IMPROVEMENT vs XGBoost')\n",
                "print(f\"{'='*50}\")\n",
                "for k in ['MAE','RMSE','MAPE','Peak_MAE']:\n",
                "    xv, hv = xgb_test_m[k], hyb_test_m[k]\n",
                "    pct = (xv - hv) / xv * 100\n",
                "    arr = 'âœ…â†“' if pct > 0 else 'âŒâ†‘'\n",
                "    print(f'  {k:10s}: XGB={xv:>8} â†’ Hybrid={hv:>8}  ({pct:+.1f}%) {arr}')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  STEP 8: WEIGHTED BLENDING (optimize alpha)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# Try different blend weights: final = alpha * hybrid + (1-alpha) * xgboost\n",
                "# to find the optimal blend on validation set\n",
                "best_alpha, best_mae = 1.0, float('inf')\n",
                "alphas_tried = np.arange(0.0, 1.05, 0.05)\n",
                "alpha_results = []\n",
                "\n",
                "for alpha in alphas_tried:\n",
                "    blended = alpha * hybrid_pred_val + (1 - alpha) * xgb_pred_val[:len(res_pred_val)]\n",
                "    m = compute_metrics(y_val_trim, blended)\n",
                "    alpha_results.append((float(alpha), m['MAE']))\n",
                "    if m['MAE'] < best_mae:\n",
                "        best_mae = m['MAE']\n",
                "        best_alpha = float(alpha)\n",
                "\n",
                "print(f'Best blend alpha: {best_alpha:.2f} (val MAE: {best_mae:.2f})')\n",
                "print(f'  alpha=1.0 means full hybrid, alpha=0.0 means pure XGBoost')\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot([a for a,_ in alpha_results], [m for _,m in alpha_results], 'bo-')\n",
                "plt.axvline(best_alpha, color='red', ls='--', label=f'Best Î±={best_alpha:.2f}')\n",
                "plt.xlabel('Alpha (blend weight)'); plt.ylabel('Val MAE')\n",
                "plt.title('Blend Weight Optimization'); plt.legend(); plt.grid(True, alpha=0.3)\n",
                "plt.savefig(os.path.join(V4_DIR, 'blend_optimization.png'), dpi=150)\n",
                "plt.show()\n",
                "\n",
                "# Apply best blend to test\n",
                "final_pred_test = best_alpha * hybrid_pred_test + (1 - best_alpha) * xgb_pred_test[:len(res_pred_test)]\n",
                "final_test_m = compute_metrics(y_test_trim, final_pred_test)\n",
                "\n",
                "final_pred_val = best_alpha * hybrid_pred_val + (1 - best_alpha) * xgb_pred_val[:len(res_pred_val)]\n",
                "final_val_m = compute_metrics(y_val_trim, final_pred_val)\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f'   FINAL (Optimally Blended) â€” Test: {final_test_m}')\n",
                "print(f\"{'='*50}\")\n",
                "\n",
                "print(f\"\\n  FINAL IMPROVEMENT vs XGBoost:\")\n",
                "for k in ['MAE','RMSE','MAPE','Peak_MAE']:\n",
                "    xv, fv = xgb_test_m[k], final_test_m[k]\n",
                "    pct = (xv - fv) / xv * 100\n",
                "    arr = 'âœ…â†“' if pct > 0 else 'âŒâ†‘'\n",
                "    print(f'    {k:10s}: XGB={xv:>8} â†’ Final={fv:>8}  ({pct:+.1f}%) {arr}')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  HORIZON-WISE ERROR\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "tt_inv = inv(y_test_trim)\n",
                "fp_inv = inv(final_pred_test)\n",
                "xp_inv = inv(xgb_pred_test[:len(res_pred_test)])\n",
                "\n",
                "horizons = [1, 24, 72, 168]\n",
                "hmets = {}\n",
                "for h in horizons:\n",
                "    i = h - 1\n",
                "    m_h = float(mean_absolute_error(tt_inv[:,i], fp_inv[:,i]))\n",
                "    r_h = float(np.sqrt(mean_squared_error(tt_inv[:,i], fp_inv[:,i])))\n",
                "    mk = tt_inv[:,i] != 0\n",
                "    mp_h = float(np.mean(np.abs((tt_inv[:,i][mk]-fp_inv[:,i][mk])/tt_inv[:,i][mk]))*100)\n",
                "    hmets[h] = {'MAE':round(m_h,2),'RMSE':round(r_h,2),'MAPE':round(mp_h,2)}\n",
                "\n",
                "print(f\"{'Hour':>6} {'MAE':>10} {'RMSE':>10} {'MAPE%':>10}\")\n",
                "for h,m in hmets.items(): print(f'{h:>6} {m[\"MAE\"]:>10} {m[\"RMSE\"]:>10} {m[\"MAPE\"]:>10}')\n",
                "\n",
                "# Compare horizon-wise: XGBoost vs Hybrid\n",
                "mph_hybrid = [float(mean_absolute_error(tt_inv[:,h], fp_inv[:,h])) for h in range(OUTPUT_LEN)]\n",
                "mph_xgb    = [float(mean_absolute_error(tt_inv[:,h], xp_inv[:,h])) for h in range(OUTPUT_LEN)]\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "plt.plot(range(1,OUTPUT_LEN+1), mph_xgb, 'r-', lw=1.5, label='XGBoost', alpha=0.7)\n",
                "plt.plot(range(1,OUTPUT_LEN+1), mph_hybrid, 'g-', lw=1.5, label='V4 Hybrid')\n",
                "for h in horizons:\n",
                "    plt.axvline(h, color='gray', ls=':', alpha=0.4)\n",
                "plt.xlabel('Horizon (h)'); plt.ylabel('MAE (MW)')\n",
                "plt.title('Horizon-Wise MAE: XGBoost vs V4 Hybrid'); plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.savefig(os.path.join(V4_DIR, 'horizon_wise_v4.png'), dpi=150)\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  FULL MODEL COMPARISON\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "with open(os.path.join(MODEL_DIR, 'baseline_metrics.json')) as f:\n",
                "    bl = json.load(f)\n",
                "v1, v2 = {}, {}\n",
                "try:\n",
                "    with open(os.path.join(MODEL_DIR, 'dl_metrics.json')) as f:\n",
                "        raw = json.load(f)\n",
                "    v1 = raw.get('CNN-BiLSTM-Attn', {'val':raw.get('val',{}),'test':raw.get('test',{})})\n",
                "except: pass\n",
                "try:\n",
                "    with open(os.path.join(MODEL_DIR,'v2','dl_metrics_v2.json')) as f:\n",
                "        raw2 = json.load(f)\n",
                "    v2 = {'val':raw2.get('val',{}),'test':raw2.get('test',{})}\n",
                "except: pass\n",
                "v3 = {}\n",
                "try:\n",
                "    with open(os.path.join(MODEL_DIR,'v3','dl_metrics_v3.json')) as f:\n",
                "        raw3 = json.load(f)\n",
                "    v3 = {'val':raw3.get('ensemble_val',{}),'test':raw3.get('corrected_test',{})}\n",
                "except: pass\n",
                "\n",
                "all_m = {'Persistence': bl['Persistence'], 'XGBoost': bl['XGBoost']}\n",
                "if v1: all_m['DL V1'] = v1\n",
                "if v2: all_m['DL V2'] = v2\n",
                "if v3: all_m['DL V3'] = v3\n",
                "all_m['V4 Hybrid'] = {'val': final_val_m, 'test': final_test_m}\n",
                "\n",
                "print(f\"\\n{'='*75}\")\n",
                "print(f'   FULL MODEL COMPARISON (ALL VERSIONS)')\n",
                "print(f\"{'='*75}\")\n",
                "for sp in ['val','test']:\n",
                "    print(f\"\\n--- {sp.upper()} ---\")\n",
                "    print(f\"{'Model':<18} {'MAE':>8} {'RMSE':>8} {'MAPE%':>8} {'PeakMAE':>10}\")\n",
                "    print('-'*58)\n",
                "    for n,m in all_m.items():\n",
                "        if sp in m and m[sp]:\n",
                "            t=m[sp]\n",
                "            print(f\"{n:<18} {t['MAE']:>8} {t['RMSE']:>8} {t['MAPE']:>8} {t['Peak_MAE']:>10}\")\n",
                "\n",
                "# Bar chart\n",
                "fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
                "mns = [n for n in all_m if 'test' in all_m[n] and all_m[n]['test']]\n",
                "colors = ['#e74c3c','#3498db','#95a5a6','#f39c12','#9b59b6','#2ecc71']\n",
                "for ax, mk in zip(axes, ['MAE','RMSE','MAPE','Peak_MAE']):\n",
                "    vals = [all_m[n]['test'][mk] for n in mns]\n",
                "    bars = ax.bar(range(len(mns)), vals, color=colors[:len(mns)], edgecolor='white')\n",
                "    ax.set_title(mk, fontweight='bold'); ax.set_ylabel(mk)\n",
                "    ax.set_xticks(range(len(mns))); ax.set_xticklabels(mns, rotation=25, ha='right', fontsize=8)\n",
                "    for b,v in zip(bars,vals):\n",
                "        ax.text(b.get_x()+b.get_width()/2, b.get_height()+0.5, f'{v}', ha='center', fontsize=8)\n",
                "    ax.grid(True, alpha=0.2, axis='y')\n",
                "plt.suptitle('All Models â€” Test Set', fontsize=14, fontweight='bold', y=1.03)\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(V4_DIR, 'model_comparison_v4.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  SAVE ALL ARTIFACTS\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "import pandas as pd\n",
                "\n",
                "v4_full = {\n",
                "    'xgb_only_test': xgb_test_m,\n",
                "    'hybrid_test': hyb_test_m,\n",
                "    'final_blended_val': final_val_m,\n",
                "    'final_blended_test': final_test_m,\n",
                "    'best_blend_alpha': best_alpha,\n",
                "    'horizon_wise': {str(k):v for k,v in hmets.items()},\n",
                "    'individual_val_losses': [round(float(v),5) for v in val_losses],\n",
                "    'hyperparameters': HP,\n",
                "}\n",
                "with open(os.path.join(V4_DIR, 'dl_metrics_v4.json'), 'w') as f:\n",
                "    json.dump(v4_full, f, indent=2, cls=NumpyEncoder)\n",
                "\n",
                "for i,h in enumerate(histories):\n",
                "    pd.DataFrame(h).to_csv(os.path.join(V4_DIR, f'history_model{i}.csv'), index=False)\n",
                "\n",
                "comp_save = {k:v for k,v in all_m.items() if 'test' in v and v['test']}\n",
                "with open(os.path.join(V4_DIR, 'all_model_comparison_v4.json'), 'w') as f:\n",
                "    json.dump(comp_save, f, indent=2, cls=NumpyEncoder)\n",
                "\n",
                "print('\\nâœ… All V4 artifacts saved to models/v4/')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Sample predictions: XGBoost vs Hybrid vs Actual\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
                "n = len(final_pred_test)\n",
                "for idx, ax in enumerate(axes.flat):\n",
                "    si = idx * (n // 4)\n",
                "    ax.plot(tt_inv[si], 'k-', lw=2, label='Actual')\n",
                "    ax.plot(xp_inv[si], 'r--', lw=1.2, alpha=0.7, label='XGBoost')\n",
                "    ax.plot(fp_inv[si], 'g-', lw=1.5, label='V4 Hybrid')\n",
                "    ax.set_title(f'Test #{si}'); ax.set_xlabel('Hour'); ax.set_ylabel('MW')\n",
                "    ax.legend(); ax.grid(True, alpha=0.3)\n",
                "plt.suptitle('V4 Hybrid Predictions: XGBoost vs Hybrid vs Actual', fontsize=14, y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(V4_DIR, 'sample_predictions_v4.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('ğŸ‰ V4 complete!')"
            ],
            "execution_count": null,
            "outputs": []
        }
    ]
}